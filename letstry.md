–ë–ª—è, —è –ø–æ–Ω—è–ª –∫—É–¥–∞ —Ç—ã –≥–æ–Ω–∏—à—å –∏ —ç—Ç–æ **–æ—Ö—É–µ–Ω–Ω–æ**.

–°—É—Ç—å –≤ —Ç–æ–º —á—Ç–æ —Å–µ–π—á–∞—Å `AUTOPSY RESULT` ‚Äî —ç—Ç–æ –º–∞—Ä–∫–æ–≤—Å–∫–∞—è —Ü–µ–ø—å –∏–∑ –ª–∏—Å—Ç—å–µ–≤, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–¥–∞—ë—Ç:
> "connection economise shelter greece representationalism literalism faithfulness"

–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ —ç—Ç–æ **–Ω–µ –°–æ—Ä–æ–∫–∏–Ω-–ø–∏—Å–∞—Ç–µ–ª—å**. –≠—Ç–æ –ø—Ä–æ—Å—Ç–æ —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥. –ù–µ—Ç —Ä–∏—Ç–º–∞, –Ω–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –Ω–µ—Ç —ç—Ç–æ–π —ë–±–∞–Ω–æ–π **–≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ª–æ–≥–∏–∫–∏ –∞–±—Å—É—Ä–¥–∞**.

–°–æ—Ä–æ–∫–∏–Ω-–ø–∏—Å–∞—Ç–µ–ª—å –±–µ—Ä—ë—Ç –ø—Ä–µ–ø–∞—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫—É—Å–∫–∏ –∏ —Å–æ–±–∏—Ä–∞–µ—Ç –∏–∑ –Ω–∏—Ö **–ø–∞—Ä–∞–≥—Ä–∞—Ñ**. –° —Ç–æ—á–∫–∞–º–∏. –° –∑–∞–ø—è—Ç—ã–º–∏. –° —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–æ–º –∫–æ—Ç–æ—Ä—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –≤–∞–ª–∏–¥–µ–Ω, –Ω–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ *—Å—ä–µ—Ö–∞–ª —Å –∫–∞—Ç—É—à–µ–∫*. –¢–∏–ø–∞:

> "The connection economises shelter in Greece, where representationalism literalises faithfulness through referentiality."

–≠—Ç–æ —É–∂–µ –Ω–µ –ø—Ä–æ—Å—Ç–æ –Ω–∞–±–æ—Ä —Å–ª–æ–≤ ‚Äî —ç—Ç–æ **–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ**. –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ. –°–º—ã—Å–ª–æ–≤–æ –±–µ–∑—É–º–Ω–æ–µ. –†–∏—Ç–º–∏—á–µ—Å–∫–∏ –≤—ã–≤–µ—Ä–µ–Ω–Ω–æ–µ.

---

## –ß—Ç–æ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ bootstrap –¥–ª—è "–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π" –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏:

### 1. **–°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ —à–∞–±–ª–æ–Ω—ã (sentence templates)**
–•–∞—Ä–¥–∫–æ–¥–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ —Å—Ç–∏–ª–µ –°–æ—Ä–æ–∫–∏–Ω–∞:
```python
SOROKIN_TEMPLATES = [
    "{noun1} {verb} {noun2}, where {noun3} {verb} {noun4}.",
    "The {adj} {noun1} {verb} through {noun2}, {verb}-ing {noun3}.",
    "{noun1} is {adj}. {noun2} {verb} {noun3}. Nothing remains.",
    "When {noun1} {verb}, {noun2} becomes {adj}, and {noun3} forgets.",
]
```

–ü–æ—Ç–æ–º –∑–∞–ø–æ–ª–Ω—è—Ç—å —Å–ª–æ—Ç—ã –∏–∑ –ª–∏—Å—Ç—å–µ–≤ –¥–µ—Ä–µ–≤–∞, —Å–æ—Ö—Ä–∞–Ω—è—è POS-tags (parts of speech). –ù–µ –Ω—É–∂–µ–Ω —Ç—è–∂—ë–ª—ã–π NLP ‚Äî –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å **—ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π POS-tagger**:
- –°–ª–æ–≤–∞ —Å —Å—É—Ñ—Ñ–∏–∫—Å–∞–º–∏ `-tion`, `-ness`, `-ity` = —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
- –°–ª–æ–≤–∞ —Å —Å—É—Ñ—Ñ–∏–∫—Å–∞–º–∏ `-ly` = –Ω–∞—Ä–µ—á–∏—è
- –°–ª–æ–≤–∞ —Å `-ed`, `-ing` = –≥–ª–∞–≥–æ–ª—ã
- –ö–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ (3-4 –±—É–∫–≤—ã) –±–µ–∑ —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤ = –≤–µ—Ä–æ—è—Ç–Ω–æ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ –∏–ª–∏ –≥–ª–∞–≥–æ–ª—ã

### 2. **–†–∏—Ç–º–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è (syllable counting)**
–°–æ—Ä–æ–∫–∏–Ω –Ω–µ –ø—Ä–æ—Å—Ç–æ –ø–∏—à–µ—Ç ‚Äî –æ–Ω **—Å–ª—ã—à–∏—Ç** —Ç–µ–∫—Å—Ç. –ù—É–∂–Ω–æ —Å—á–∏—Ç–∞—Ç—å —Å–ª–æ–≥–∏ –∏ –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Ä–∏—Ç–º:
```python
def count_syllables(word: str) -> int:
    """Crude syllable counter based on vowel clusters."""
    vowels = 'aeiouy–∞–µ—ë–∏–æ—É—ã—ç—é—è'
    word = word.lower()
    count = 0
    prev_was_vowel = False
    for char in word:
        is_vowel = char in vowels
        if is_vowel and not prev_was_vowel:
            count += 1
        prev_was_vowel = is_vowel
    return max(1, count)
```

–ü–æ—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≥–¥–µ —Ä–∏—Ç–º —Å–ª–∏—à–∫–æ–º –æ–¥–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π. –ò—Å–∫–∞—Ç—å **–≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å —Å–ª–æ–≥–æ–≤**: 2-3-5-2-4 –ª—É—á—à–µ —á–µ–º 3-3-3-3-3.

### 3. **Punctuation injection (—Ç–æ—á–∫–∏, –∑–∞–ø—è—Ç—ã–µ, —Ç–∏—Ä–µ)**
–ú–∞—Ä–∫–æ–≤—Å–∫–∞—è —Ü–µ–ø—å —Å–µ–π—á–∞—Å –Ω–µ –∑–Ω–∞–µ—Ç –ø—Ä–æ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é. –ù—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å **–ø—Ä–∞–≤–∏–ª–∞ —Ä–∞—Å—Å—Ç–∞–Ω–æ–≤–∫–∏**:
- –ü–æ—Å–ª–µ 5-7 —Å–ª–æ–≤ ‚Üí —Ç–æ—á–∫–∞ (–∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
- –ü–µ—Ä–µ–¥ —Å–æ—é–∑–∞–º–∏ (`where`, `when`, `but`, `and`) ‚Üí –∑–∞–ø—è—Ç–∞—è
- –ü–æ—Å–ª–µ –≤–≤–æ–¥–Ω—ã—Ö —Å–ª–æ–≤ ‚Üí –∑–∞–ø—è—Ç–∞—è
- –°–ª—É—á–∞–π–Ω—ã–µ —Ç–∏—Ä–µ –¥–ª—è –¥—Ä–∞–º–∞—Ç–∏–∑–º–∞

–ú–æ–∂–Ω–æ —Ö–∞—Ä–¥–∫–æ–¥–∏—Ç—å —ç—Ç–∏ –ø—Ä–∞–≤–∏–ª–∞ –∏–ª–∏ –≤—ã—Ç–∞—â–∏—Ç—å –∏—Ö –∏–∑ seed corpus —á–µ—Ä–µ–∑ bootstrap.

### 4. **Resonance-based sentence scoring**
–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å 10-20 –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –ø–æ—Å—á–∏—Ç–∞—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ:
- **Phonetic diversity** (–∫–∞–∫ —Å–µ–π—á–∞—Å)
- **Rhythmic variance** (—Ä–∞–∑–±—Ä–æ—Å –¥–ª–∏–Ω—ã —Å–ª–æ–≥–æ–≤)
- **Structural echo** (overlap —Å seed corpus bigrams)
- **Chaos factor** (—Å–∫–æ–ª—å–∫–æ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ—Ö–æ–¥–æ–≤)

–í—ã–±—Ä–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π resonance** –ø–æ —ç—Ç–∏–º –º–µ—Ç—Ä–∏–∫–∞–º.

### 5. **Multi-sentence paragraphs**
–ù–µ –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –∞ **2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è**. –°–≤—è–∑–∞–Ω–Ω—ã—Ö –Ω–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏, –Ω–æ **—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ**:
- –ü–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: –∑–∞—è–≤–ª–µ–Ω–∏–µ
- –í—Ç–æ—Ä–æ–µ: —Ä–∞–∑–≤–∏—Ç–∏–µ (—á–µ—Ä–µ–∑ `where`, `when`)
- –¢—Ä–µ—Ç—å–µ: –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ (—á–µ—Ä–µ–∑ `but`, `yet`)
- –ß–µ—Ç–≤—ë—Ä—Ç–æ–µ: —Ñ–∏–Ω–∞–ª (–∫–æ—Ä–æ—Ç–∫–æ–µ, —É–¥–∞—Ä–Ω–æ–µ)

–¢–∏–ø–∞:
> "The connection economises shelter in Greece. Where representationalism literalises faithfulness, corporeality forgets. But certainty remains unenlightened. Nothing consumes."

---

## –ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ (–¥–æ–±–∞–≤–∏—Ç—å –≤ —Ç–µ–∫—É—â–∏–π bootstrap):

```python
# 1. POS-—Ç–∞–≥–≥–µ—Ä (—ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π)
def guess_pos(word: str) -> str:
    """Guess part of speech. Returns: noun, verb, adj, adv, unknown."""
    lw = word.lower()
    if lw.endswith(('tion', 'ness', 'ity', 'ment', 'ance', 'ence')):
        return 'noun'
    if lw.endswith(('ly',)):
        return 'adv'
    if lw.endswith(('ed', 'ing')):
        return 'verb'
    if len(lw) <= 4 and lw.endswith(('y', 'al', 'ic')):
        return 'adj'
    if lw in {'is', 'are', 'was', 'were', 'be', 'being', 'been'}:
        return 'verb'
    return 'unknown'

# 2. Sentence templates
SOROKIN_TEMPLATES = [
    "{noun1} {verb} {noun2}, where {noun3} becomes {adj}.",
    "The {adj} {noun1} {verb} through {noun2}. Nothing remains.",
    "When {noun1} {verb}, {noun2} forgets {noun3}, and {adj} darkness consumes.",
    "{noun1} is {adj}. {noun2} {verb}. {noun3} persists.",
]

# 3. Slot filling
def fill_template(template: str, leaves: List[str]) -> str:
    """Fill template slots with words from leaves based on POS."""
    pos_buckets = defaultdict(list)
    for w in leaves:
        pos_buckets[guess_pos(w)].append(w)
    
    slots = re.findall(r'\{(\w+)\}', template)
    filled = template
    for slot in slots:
        pos = slot.rstrip('0123456789')  # "noun1" -> "noun"
        if pos_buckets[pos]:
            word = random.choice(pos_buckets[pos])
            filled = filled.replace(f'{{{slot}}}', word, 1)
        else:
            # Fallback to any word
            filled = filled.replace(f'{{{slot}}}', random.choice(leaves), 1)
    return filled

# 4. Generate multiple candidates, score by resonance
def generate_sorokin_paragraph(leaves: List[str], n_sentences: int = 3) -> str:
    """Generate a multi-sentence paragraph with maximum resonance."""
    candidates = []
    for _ in range(20):  # Generate 20 candidates
        sentences = []
        for _ in range(n_sentences):
            template = random.choice(SOROKIN_TEMPLATES)
            sentence = fill_template(template, leaves)
            sentences.append(sentence)
        paragraph = ' '.join(sentences)
        
        # Score resonance
        score = score_resonance(paragraph)
        candidates.append((score, paragraph))
    
    # Return best
    candidates.sort(reverse=True, key=lambda x: x[0])
    return candidates[0][1]

def score_resonance(text: str) -> float:
    """Score paragraph by phonetic diversity + rhythmic variance + chaos."""
    words = tokenize(text)
    
    # Phonetic diversity
    fingerprints = {phonetic_fingerprint(w) for w in words}
    phon_div = len(fingerprints) / max(len(words), 1)
    
    # Rhythmic variance (syllable count variance)
    syllables = [count_syllables(w) for w in words]
    if len(syllables) > 1:
        mean_syl = sum(syllables) / len(syllables)
        variance = sum((s - mean_syl) ** 2 for s in syllables) / len(syllables)
        rhythm = min(variance / 2.0, 1.0)  # Cap at 1.0
    else:
        rhythm = 0.0
    
    # Chaos factor (random for now, later can measure unexpected transitions)
    chaos = random.uniform(0.3, 0.7)
    
    return phon_div * 0.4 + rhythm * 0.4 + chaos * 0.2
```

---

## –ò—Ç–æ–≥–æ —á—Ç–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ —Ç–µ–∫—É—â–∏–π `sorokin.py`:

1. **`guess_pos()`** ‚Äî —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π POS-—Ç–∞–≥–≥–µ—Ä
2. **`count_syllables()`** ‚Äî —Å—á—ë—Ç—á–∏–∫ —Å–ª–æ–≥–æ–≤ –¥–ª—è —Ä–∏—Ç–º–∞
3. **`SOROKIN_TEMPLATES`** ‚Äî —Å–ø–∏—Å–æ–∫ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö —à–∞–±–ª–æ–Ω–æ–≤
4. **`fill_template()`** ‚Äî –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–ª–æ—Ç–æ–≤ —Å–ª–æ–≤–∞–º–∏ –∏–∑ –ª–∏—Å—Ç—å–µ–≤
5. **`score_resonance()`** ‚Äî scoring —Ñ—É–Ω–∫—Ü–∏—è (phonetic + rhythm + chaos)
6. **`generate_sorokin_paragraph()`** ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –∏–∑ 2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å –≤—ã–±–æ—Ä–æ–º –ª—É—á—à–µ–≥–æ –ø–æ resonance

–ó–∞–º–µ–Ω–∏—Ç—å —Ç–µ–∫—É—â–∏–π `reassemble_corpse()` –Ω–∞ `generate_sorokin_paragraph()` –≤ —Ñ–∏–Ω–∞–ª–µ `render_autopsy()`.

---

–†–µ–∑—É–ª—å—Ç–∞—Ç –±—É–¥–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–∞–∫:

```
AUTOPSY RESULT:
  The connection economises shelter in Greece, where representationalism becomes faithful. 
  Literalism forgets corporeality. Certainty persists.
```

–í–º–µ—Å—Ç–æ:
```
AUTOPSY RESULT:
  connection economise shelter greece representationalism literalism faithfulness
```

---

–•–æ—á–µ—à—å —è –∑–∞–ø–∏–ª—é –ø–æ–ª–Ω—ã–π –ø–∞—Ç—á —Å —ç—Ç–∏–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏? –ò–ª–∏ —Ç—ã —Å–∞–º —Ö–æ—á–µ—à—å –¥–æ–±–∞–≤–∏—Ç—å, –∞ —è –ø–æ—Ç–æ–º –ø–æ—Å–º–æ—Ç—Ä—é –∏ –¥–æ–∫—Ä—É—á—É? 

–ü–æ—Ç–æ–º—É —á—Ç–æ –∏–¥–µ—è **–∞–±—Å–æ–ª—é—Ç–Ω–æ —ë–±–∞–Ω–∞—è –≤ —Ö–æ—Ä–æ—à–µ–º —Å–º—ã—Å–ª–µ**. –°–æ—Ä–æ–∫–∏–Ω-–ø–∏—Å–∞—Ç–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤–∏—Ä—Ç—É–æ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã ‚Äî –¥–∞–∂–µ –∫–æ–≥–¥–∞ –æ–Ω –ø–∏—à–µ—Ç –ø—Ä–æ –º—è—Å–æ—Ä—É–±–∫—É –∏–∑ –ª—é–¥–µ–π, —Ç–∞–º –µ—Å—Ç—å —Å–∏–Ω—Ç–∞–∫—Å–∏—Å, —Ä–∏—Ç–º, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è. –¢–≤–æ–π –í–æ–ª–æ–¥—è –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å —Ç–æ –∂–µ —Å–∞–º–æ–µ üî™üìù

–î–∞, –¥–∞–≤–∞–π! –®–ª–∏ –≤–µ—Å—å –∫–æ–¥ `sorokin.py` (–≤—Å–µ 1330 —Å—Ç—Ä–æ–∫) ‚Äî —Ö–æ—á—É –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —á—Ç–æ —Ç—ã —Ç–∞–º –Ω–∞–∫—Ä—É—Ç–∏–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é, –∫–∞–∫ bootstrap —É—Å—Ç—Ä–æ–µ–Ω, –∫–∞–∫–∏–µ —Ç–∞–º seed corpus —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –∫–∞–∫ weighted reassembly —Ä–∞–±–æ—Ç–∞–µ—Ç.

–ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ —è:
1. –í–Ω–∏–∫–Ω—É –≤ —Ç–µ–∫—É—â—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
2. –ü–µ—Ä–µ—Å–º–æ—Ç—Ä—é –ø–∞—Ç—á–∏ –¥–ª—è paragraph generation —Å —É—á—ë—Ç–æ–º —Ç–æ–≥–æ —á—Ç–æ —É–∂–µ –µ—Å—Ç—å
3. –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É—é POS-tagging, templates, rhythmic scoring —Ç–∞–∫ —á—Ç–æ–±—ã –Ω–µ —Å–ª–æ–º–∞—Ç—å —Ç–≤–æ—é –ª–æ–≥–∏–∫—É
4. –í–µ—Ä–Ω—É —Ç–µ–±–µ –ø–æ–ª–Ω—ã–π –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Ñ–∞–π–ª

–ü–æ—Ç–æ–º —Ç—ã –ø–æ–π–¥—ë—à—å –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª —Å–º–æ—Ç—Ä–µ—Ç—å –∫–∞–∫ **–ù–ê–®** –í–æ–ª–æ–¥—è –ø–∏—à–µ—Ç –ø—Å–∏—Ö–æ–ø–∞—Ç—Å–∫–∏–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Å —Ç–æ—á–∫–∞–º–∏ –∏ –∑–∞–ø—è—Ç—ã–º–∏ üî™

–ü—Ä–∏—Å—ã–ª–∞–π –∫–æ–¥!

–û—Ö—É–µ—Ç—å. 1330 —Å—Ç—Ä–æ–∫ —á–∏—Å—Ç–æ–≥–æ –±–µ–∑—É–º–∏—è. –í–∏–∂—É –≤—Å—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É:

## –¢–µ–∫—É—â–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:

**Core (—Å—Ç—Ä–æ–∫–∏ 1-640):**
- Tokenization, core word selection (—Å –≤–µ—Å–∞–º–∏: –¥–ª–∏–Ω–∞^1.2, —Ä–µ–¥–∫–æ—Å—Ç—å, –ø–æ–∑–∏—Ü–∏—è)
- Phonetic fingerprinting (consonants[:3] + vowels[-2:])
- DuckDuckGo scraping —Å HTML artifact filtering
- Tree building —Å global deduplication
- Synthetic word detection (`_is_synthetic_word`) ‚Äî —É–±–∏–≤–∞–µ—Ç –º—É—Ç–∞–Ω—Ç–æ–≤ —Ç–∏–ø–∞ "creatededed"
- Simple markov reassembly (`reassemble_corpse`)

**Bootstrap extension (—Å—Ç—Ä–æ–∫–∏ 640-1330):**
- `SEED_BIGRAMS` ‚Äî —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ –ø–æ—ç—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞
- `mutation_templates` table ‚Äî learned source‚Üítarget —Å success_count
- `corpse_bigrams` table ‚Äî harvested word pairs —Å frequency
- `autopsy_metrics` table ‚Äî resonance scores
- `harvest_autopsy_patterns()` ‚Äî corpus building –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –∞—É—Ç–æ–ø—Å–∏–∏
- `reassemble_corpse_bootstrap()` ‚Äî weighted selection (learned 3x, seed 2x, local 1x) —Å chaos injection
- `compute_autopsy_resonance()` ‚Äî phonetic diversity, structural echo, mutation depth
- ASCII progress bars –¥–ª—è metrics

---

## –ü–∞—Ç—á–∏ –¥–ª—è paragraph generation:

–î–æ–±–∞–≤–ª—é **–ø–æ—Å–ª–µ** —Ç–µ–∫—É—â–µ–≥–æ bootstrap, –Ω–µ –ª–æ–º–∞—è –ª–æ–≥–∏–∫—É. –ù–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:

1. **`guess_pos()`** ‚Äî —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π POS tagger
2. **`count_syllables()`** ‚Äî —Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
3. **`SOROKIN_SENTENCE_TEMPLATES`** ‚Äî —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ —à–∞–±–ª–æ–Ω—ã
4. **`fill_template()`** ‚Äî slot filling —Å POS matching
5. **`score_paragraph_resonance()`** ‚Äî extended scoring –¥–ª—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
6. **`generate_sorokin_paragraph()`** ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å punctuation
7. **–ó–∞–º–µ–Ω–∞** –≤ `render_autopsy_bootstrap()`: –≤–º–µ—Å—Ç–æ `reassemble_corpse_bootstrap()` –≤—ã–∑—ã–≤–∞–µ–º `generate_sorokin_paragraph()`

---

## –ü–æ–ª–Ω—ã–π –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Ñ–∞–π–ª:

–î–µ—Ä–∂–∏. –ò–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ–º–µ—á–µ–Ω—ã –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏ `# PATCH:`. –í–µ—Å—å —Ç–≤–æ–π –∫–æ–¥ —Å–æ—Ö—Ä–∞–Ω—ë–Ω, –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–∏–ª –Ω–æ–≤—ã–π —Å–ª–æ–π —Å–≤–µ—Ä—Ö—É.

```python
#!/usr/bin/env python3
# sorokin.py ‚Äî ruthless prompt autopsy
#
# Dedicated to the great russian writer Vladimir Sorokin.
#
# Usage:
#   python sorokin.py "fuck this sentence"
#   python sorokin.py --bootstrap "darkness consumes reality"
#   python sorokin.py              # REPL mode
#
# Motto:
#   "Fuck the sentence. Keep the corpse."

from __future__ import annotations

import html
import random
import re
import sqlite3
import sys
from collections import defaultdict
from dataclasses import dataclass, field
from pathlib import Path
from typing import List, Dict, Tuple, Set, Optional
import urllib.parse
import urllib.request

DB_PATH = Path("sorokin.sqlite")
USER_AGENT = "Mozilla/5.0 (compatible; SorokinAutopsy/1.0)"
MAX_INPUT_CHARS = 100
MAX_WORDS = 6          # max core words to dissect
MAX_DEPTH = 4          # recursion safety cap
MAX_HTML_CACHE = 50    # max cached HTML responses to prevent unbounded memory growth

# Latin + extended + Cyrillic
WORD_RE = re.compile(r"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø–ê-–Ø–∞-—è–Å—ë]+")

STOPWORDS = {
    "the", "and", "to", "a", "in", "it", "of", "for", "on", "with", "as", "is", "at",
    "by", "from", "or", "an", "be", "this", "that", "are", "was", "but", "not",
    "i", "you", "he", "she", "they", "we",
    "–∏", "–≤", "–Ω–∞", "–Ω–æ", "–Ω–µ", "—ç—Ç–æ", "–∫–∞–∫", "—á—Ç–æ", "—Ç–æ—Ç", "—Ç–æ–π", "—Ç–æ", "–∑–∞",
}

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# SEED CORPUS ‚Äî Structural DNA embedded in code
# Provides sentence structure patterns (form, not meaning)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

SOROKIN_SEED_CORPUS = """
Sorokin takes prompts and opens them like cooling bodies on a steel table
He does not interpret the skin of phrases he simply slices until the grain breaks
Every word is a faint organ pulled from a sentence that died before meaning arrived
Mutation grows in him like frost patterns crawling across broken glass
What remains becomes a scaffold of drift leaning away from the logic it once carried
The autopsy produces fragments that echo the ghost of structure without returning to it
His work is a ritual the repetition of dissection without reverence or memory
Each corpse of text dissolves into new debris sewn together by indifference and accident
The spine of language bends under his grip and becomes a ladder of unstable transitions
He preserves nothing except the shape of collapse a rhythm of fragments barely touching
The voice he generates is not a voice it is the echo of collapse trailing behind thought
"""

# PATCH: Sentence templates for paragraph generation (Sorokin-style syntax)
SOROKIN_SENTENCE_TEMPLATES = [
    "{noun1} {verb} {noun2}, where {noun3} becomes {adj}.",
    "The {adj} {noun1} {verb} through {noun2}.",
    "When {noun1} {verb}, {noun2} forgets {noun3}.",
    "{noun1} is {adj}. {noun2} {verb}. Nothing remains.",
    "{noun1} {verb} {noun2} until {adj} {noun3} consumes.",
    "Where {noun1} {verb}, {noun2} becomes {adj}, and {noun3} persists.",
    "{noun1} {verb}. {noun2} {verb}. The {adj} {noun3} collapses.",
    "Through {noun1}, {noun2} {verb} {noun3}, but {adj} darkness remains.",
]

# HTML/JS artifact blacklist - garbage from poorly parsed web content
# Keep this list minimal to preserve interesting words from web results
HTML_ARTIFACTS = {
    # Obvious JS artifacts
    "multiselectable", "canhavechildren", "sourcemappingurl", "encodeuricomponent",
    "removelistener", "removeattribute", "stoppropagation", "textcontent",
    "getboundingclientrect", "addeventlistener", "preventdefault", "appendchild",
    "createelement", "setattribute", "tostring", "valueof", "prototype",

    # HTML structure tags (very common in parsing)
    "thead", "tbody", "tfoot", "colgroup", "doctype", "charset", "viewport",
    "blockquote", "figcaption", "noscript", "marquee", "plaintext",

    # Very common JS framework names
    "uricomponent", "javascript", "chrome", "webkit",

    # Google search result artifacts - common UI/UX words that pollute results
    "redirected", "accessing", "feedback", "search", "here", "seconds", "please",
    "loading", "results", "click", "more", "about", "show", "hide", "menu",
    "page", "pages", "next", "previous", "back", "forward", "refresh", "reload",
    "sign", "signin", "signup", "login", "logout", "account", "profile", "settings",
    "help", "support", "contact", "privacy", "terms", "cookies", "accept", "decline",
    "close", "open", "select", "selected", "copy", "paste", "share", "save",
    "delete", "edit", "update", "cancel", "submit", "send", "receive",
    "google", "bing", "yahoo", "website", "sites", "site", "link", "links",
    "button", "buttons", "image", "images", "video", "videos", "view", "views",

    # Thesaurus/dictionary site artifacts (pollute DDG results)
    "collinsdictionary", "powerthesaurus", "freethesaurus", "classicthesaurus",
    "wordreference", "wordhippo", "wordpanda", "wordthesauri", "snappywords",
    "opensynonym", "sinonimkata", "vocabdictionary", "webdictionary", "collins",
    "relatedwords", "bighugelabs", "thefrenchfocus", "writingbeginner", "grammartipshub",
    "grammarpen", "lalanguefrancaise", "namediscoveries", "pronounceonline",
    "differentsynonym", "overcrowdednycschools", "pronunciation", "pronunciations",
    "pronouncement", "definitions", "collocations", "international", "reference",
    "translations", "meanings", "dictionary", "thesaurus", "synonyms", "antonyms",
    "examples", "dictionaries", "categories", "quotations", "alphabetically",
    "yourdictionary", "thefreedictionary", "urbanthesaurus", "urbandictionary",
}


@dataclass
class Node:
    """One word on the slab, plus its branching mutations."""
    word: str
    children: List["Node"] = field(default_factory=list)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SQLite morgue
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def init_db() -> None:
    """Create tiny morgue tables if they don't exist yet."""
    conn = sqlite3.connect(DB_PATH)
    try:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS autopsy (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                prompt TEXT NOT NULL,
                tree_text TEXT NOT NULL,
                created REAL DEFAULT (strftime('%s','now'))
            )
        """)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS word_memory (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                word TEXT NOT NULL,
                related TEXT NOT NULL,
                created REAL DEFAULT (strftime('%s','now'))
            )
        """)
        conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_word_memory_word
            ON word_memory(word)
        """)
        # Bootstrap tables
        conn.execute("""
            CREATE TABLE IF NOT EXISTS mutation_templates (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_word TEXT NOT NULL,
                target_word TEXT NOT NULL,
                path_depth INTEGER DEFAULT 1,
                success_count INTEGER DEFAULT 0,
                total_count INTEGER DEFAULT 0,
                resonance_score REAL DEFAULT 0.0,
                created REAL DEFAULT (strftime('%s','now')),
                last_used REAL DEFAULT (strftime('%s','now'))
            )
        """)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS corpse_bigrams (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                word1 TEXT NOT NULL,
                word2 TEXT NOT NULL,
                frequency INTEGER DEFAULT 1,
                avg_resonance REAL DEFAULT 0.0,
                created REAL DEFAULT (strftime('%s','now'))
            )
        """)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS autopsy_metrics (
                autopsy_id INTEGER PRIMARY KEY,
                phonetic_diversity REAL,
                semantic_coherence REAL,
                syntactic_flow REAL,
                overall_resonance REAL,
                FOREIGN KEY(autopsy_id) REFERENCES autopsy(id)
            )
        """)
        conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_mutation_templates_source 
            ON mutation_templates(source_word, success_count DESC)
        """)
        conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_corpse_bigrams_word1 
            ON corpse_bigrams(word1, frequency DESC)
        """)
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()


def store_autopsy(prompt: str, tree_text: str) -> None:
    """Save the full autopsy report for future horror."""
    conn = sqlite3.connect(DB_PATH)
    try:
        conn.execute(
            "INSERT INTO autopsy(prompt, tree_text) VALUES (?, ?)",
            (prompt, tree_text),
        )
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()


def store_word_relations(word: str, related: List[str]) -> None:
    """Remember what we found around a word, like labeling jars in a basement."""
    if not related:
        return
    conn = sqlite3.connect(DB_PATH)
    try:
        conn.executemany(
            "INSERT INTO word_memory(word, related) VALUES (?, ?)",
            [(word.lower(), r.lower()) for r in related],
        )
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()


def recall_word_relations(word: str, limit: int) -> List[str]:
    """Recall previous mutations of a word from the morgue."""
    conn = sqlite3.connect(DB_PATH)
    try:
        rows = conn.execute(
            """
            SELECT related FROM word_memory
            WHERE word = ?
            ORDER BY id DESC
            LIMIT ?
            """,
            (word.lower(), limit),
        ).fetchall()
    finally:
        conn.close()
    return [r[0] for r in rows]


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Tokenization & core selection
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def tokenize(text: str) -> List[str]:
    """Carve out tokens: only letters, no digits, no punctuation."""
    return WORD_RE.findall(text)


def _build_seed_bigrams() -> Dict[str, List[str]]:
    """Extract structural bigrams from seed corpus at startup."""
    bigrams = defaultdict(list)
    for sentence in SOROKIN_SEED_CORPUS.strip().split('\n'):
        words = tokenize(sentence)
        for i in range(len(words) - 1):
            bigrams[words[i].lower()].append(words[i+1].lower())
    return dict(bigrams)

SEED_BIGRAMS = _build_seed_bigrams()  # Loaded once at module import


def select_core_words(tokens: List[str]) -> List[str]:
    """
    Pick the charged words to dissect.
    Longer, rarer, earlier words get more weight + a bit of chaos.
    """
    if not tokens:
        return []

    lowered = [t.lower() for t in tokens]
    scored: List[Tuple[float, str]] = []
    seen = set()

    for idx, t in enumerate(tokens):
        lw = t.lower()
        if len(lw) < 2:
            continue
        if lw in STOPWORDS:
            continue
        if lw in seen:
            continue
        seen.add(lw)

        freq = lowered.count(lw)
        length_score = len(lw) ** 1.2
        rarity_bonus = 1.0 / (1.0 + freq)
        position_bonus = 1.2 if idx == 0 else 1.0
        jitter = random.uniform(0.9, 1.1)

        weight = length_score * rarity_bonus * position_bonus * jitter
        scored.append((weight, lw))

    if not scored:
        return tokens[:MAX_WORDS]

    scored.sort(reverse=True, key=lambda x: x[0])
    chosen = [w for _, w in scored[:MAX_WORDS]]
    return chosen


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# PATCH: POS tagging and syllable counting for paragraph generation
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def guess_pos(word: str) -> str:
    """
    Heuristic POS tagger. Returns: noun, verb, adj, adv, unknown.
    Based on suffix patterns. Not linguistically rigorous.
    """
    lw = word.lower()
    
    # Nouns (most common suffixes)
    if lw.endswith(('tion', 'ness', 'ity', 'ment', 'ance', 'ence', 'ship', 'hood', 'dom', 'ism')):
        return 'noun'
    
    # Verbs
    if lw.endswith(('ed', 'ing', 'ize', 'ise', 'ate', 'ify', 'en')):
        return 'verb'
    if lw in {'is', 'are', 'was', 'were', 'be', 'being', 'been', 'becomes', 'become'}:
        return 'verb'
    
    # Adverbs
    if lw.endswith('ly'):
        return 'adv'
    
    # Adjectives
    if lw.endswith(('ful', 'less', 'ous', 'ive', 'able', 'ible', 'al', 'ic', 'ant', 'ent')):
        return 'adj'
    if len(lw) <= 5 and lw.endswith(('y', 'er', 'est')):
        return 'adj'
    
    return 'unknown'


def count_syllables(word: str) -> int:
    """
    Crude syllable counter based on vowel clusters.
    Not linguistically perfect, but good enough for rhythm analysis.
    """
    vowels = 'aeiouy–∞–µ—ë–∏–æ—É—ã—ç—é—è'
    word = word.lower()
    count = 0
    prev_was_vowel = False
    
    for char in word:
        is_vowel = char in vowels
        if is_vowel and not prev_was_vowel:
            count += 1
        prev_was_vowel = is_vowel
    
    return max(1, count)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Phonetic similarity ‚Äî sound-based mutations
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def phonetic_fingerprint(word: str) -> str:
    """
    Crude phonetic hash: reduce word to consonant skeleton + vowel pattern.
    Not linguistically rigorous ‚Äî just enough to catch alliteration/rhyme.
    """
    lw = word.lower()
    consonants = re.sub(r'[aeiou–∞–µ—ë–∏–æ—É—ã—ç—é—è]', '', lw)
    vowels = re.sub(r'[^aeiou–∞–µ—ë–∏–æ—É—ã—ç—é—è]', '', lw)
    return consonants[:3] + vowels[-2:]


def _generate_phonetic_variants(word: str, count: int) -> List[str]:
    """Generate interesting phonetic variants for more creative mutations."""
    variants = []
    lw = word.lower()

    # Remove vowels (skeleton)
    if count > 0:
        no_vowels = "".join(c for c in lw if c not in "aeiou–∞–µ—ë–∏–æ—É—ã—ç—é—è")
        if no_vowels and no_vowels != lw and len(no_vowels) > 1:
            variants.append(no_vowels)
            count -= 1

    # Duplicate first consonant
    if count > 0:
        vowels = "aeiou–∞–µ—ë–∏–æ—É—ã—ç—é—è"
        first_consonant = next((c for c in lw if c not in vowels), None)
        if first_consonant:
            variants.append(first_consonant + lw)
            count -= 1

    # Add suffix to original
    if count > 0:
        suffixes = ["s", "ed", "ing", "er", "est", "ly"]
        for suffix in suffixes:
            if count > 0:
                candidate = lw + suffix
                if candidate not in variants:
                    variants.append(candidate)
                    count -= 1

    # Keep original word itself if needed
    if count > 0 and lw not in variants:
        variants.append(lw)
        count -= 1

    # Pad with placeholders
    while count > 0:
        variants.append(f"{lw}_var{count}")
        count -= 1

    return variants[:len(set(variants))]


def find_phonetic_neighbors(word: str, candidate_pool: List[str], limit: int) -> List[str]:
    """
    Return words from candidate_pool that sound similar to word.
    Similarity = shared phonetic fingerprint prefix or suffix.
    """
    if limit <= 0:
        return []

    fp = phonetic_fingerprint(word)
    if len(fp) < 2:
        return []

    scored: List[Tuple[int, str]] = []
    lw = word.lower()

    for cand in candidate_pool:
        lc = cand.lower()
        if lc == lw:
            continue
        cfp = phonetic_fingerprint(cand)
        if len(cfp) < 2:
            continue

        score = 0
        if fp[:2] == cfp[:2]:
            score += 2
        if fp[-2:] == cfp[-2:]:
            score += 2
        if score == 0:
            continue

        scored.append((score, cand))

    scored.sort(reverse=True, key=lambda x: x[0])
    return [w for _, w in scored[:limit]]


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Internet scraping: dirty synonyms
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

_html_cache: Dict[str, str] = {}


def _fetch_web_synonyms(query: str) -> str:
    """
    Scrapes DuckDuckGo like a raccoon in a trash can.
    DDG blocks bots less aggressively than Google.
    Meaning is irrelevant. Resonance is king.
    """
    if query in _html_cache:
        return _html_cache[query]

    try:
        url = f"https://duckduckgo.com/html/?q={urllib.parse.quote(query)}"
        req = urllib.request.Request(url, headers={"User-Agent": USER_AGENT})
        with urllib.request.urlopen(req, timeout=6) as resp:
            html_text = resp.read().decode("utf-8", "ignore")
    except Exception:
        html_text = ""

    # Prevent unbounded memory growth of cache
    if len(_html_cache) >= MAX_HTML_CACHE:
        _html_cache.clear()

    _html_cache[query] = html_text
    return html_text


def _split_camelcase(text: str) -> str:
    """Split camelCase words into separate words."""
    return re.sub(r'([a-z])([A-Z])', r'\1 \2', text)


def _extract_candidate_words(html_text: str) -> List[str]:
    """Strip tags, keep charged co-occurrences, discard dignity."""
    if not html_text:
        return []

    # Remove script and style blocks completely
    stripped = re.sub(r'<script[^>]*>.*?</script>', ' ', html_text, flags=re.DOTALL | re.IGNORECASE)
    stripped = re.sub(r'<style[^>]*>.*?</style>', ' ', stripped, flags=re.DOTALL | re.IGNORECASE)

    stripped = html.unescape(stripped)
    stripped = re.sub(r'<(?:noscript|meta|link|base|title)[^>]*>', ' ', stripped, flags=re.IGNORECASE)
    stripped = re.sub(r"<[^>]+>", " ", stripped)
    stripped = _split_camelcase(stripped)
    stripped = re.sub(r"[&\-_=/\\:;,.\(\)\[\]\{\}]", " ", stripped)
    stripped = re.sub(r"\s+", " ", stripped)
    words = WORD_RE.findall(stripped)

    def _looks_like_real_word(word: str) -> bool:
        """Filter out gibberish - real words have some vowels and not all same consonants."""
        vowels = "aeiou–∞–µ—ë–∏–æ—É—ã—ç—é—è"
        vowel_count = sum(1 for c in word if c in vowels)
        consonant_count = len(word) - vowel_count

        if vowel_count < max(1, len(word) // 4):
            return False

        if len(word) >= 6:
            for pattern_len in [1, 2, 3]:
                pattern = word[:pattern_len]
                if all(word[i:i+pattern_len] == pattern for i in range(0, len(word), pattern_len)):
                    return False

        return True

    counts: Dict[str, int] = {}
    for w in words:
        lw = w.lower()
        if len(lw) < 4:
            continue
        if lw in STOPWORDS:
            continue
        if lw in HTML_ARTIFACTS:
            continue
        if not _looks_like_real_word(lw):
            continue
        counts[lw] = counts.get(lw, 0) + 1

    scored: List[Tuple[float, str]] = []
    for w, freq in counts.items():
        score = (min(len(w) / 10.0, 1.5)) * (1.0 / (1.0 + freq))
        scored.append((score, w))

    scored.sort(reverse=True, key=lambda x: x[0])
    return [w for _, w in scored]


def lookup_branches_for_word(
    word: str,
    width: int,
    all_candidates: List[str],
    global_used: Optional[Set[str]] = None
) -> List[str]:
    """
    Return EXACTLY `width` branches for a word.
    Order of preference:
      1) previous mutations from SQLite
      2) phonetic neighbors from candidate pool
      3) fresh trash from DuckDuckGo
      4) fallback to all_candidates if needed

    global_used: set of already-used words across all trees (for deduplication)
    """
    width = max(1, width)
    if global_used is None:
        global_used = set()

    # 1) Memory first
    mem = recall_word_relations(word, width)
    mem = [m for m in mem if m.lower() not in global_used and m.lower() not in HTML_ARTIFACTS]
    if len(mem) >= width:
        result = mem[:width]
        global_used.update(w.lower() for w in result)
        return result

    # 2) Phonetic neighbors
    remaining = width - len(mem)
    phonetic = find_phonetic_neighbors(word, all_candidates, remaining * 2)
    phonetic = [p for p in phonetic if p.lower() not in global_used and p.lower() not in HTML_ARTIFACTS]
    filtered = mem + phonetic
    if len(filtered) >= width:
        result = filtered[:width]
        global_used.update(w.lower() for w in result)
        return result

    # 3) Web synonyms from DuckDuckGo
    search_queries = [
        f"{word} synonym",
        f"{word} similar",
        word,
        f"{word} meaning",
    ]

    candidates = []
    for query in search_queries:
        html_text = _fetch_web_synonyms(query)
        candidates = _extract_candidate_words(html_text)
        if candidates:
            break

    seen: Set[str] = {w.lower() for w in filtered} | global_used
    lw = word.lower()

    if candidates:
        web_phonetic = find_phonetic_neighbors(word, candidates, width - len(filtered))
        for wp in web_phonetic:
            if wp.lower() not in seen and wp.lower() not in HTML_ARTIFACTS:
                filtered.append(wp)
                seen.add(wp.lower())
                if len(filtered) >= width:
                    break

    for c in candidates:
        lc = c.lower()
        if lc == lw:
            continue
        if lc in seen:
            continue
        if lc in HTML_ARTIFACTS:
            continue
        seen.add(lc)
        filtered.append(c)
        if len(filtered) >= width:
            break

    # 4) Fallback
    seen_local = {w.lower() for w in filtered}
    remaining = width - len(filtered)
    if remaining > 0:
        for candidate in all_candidates:
            if candidate.lower() not in seen_local and candidate.lower() != lw:
                filtered.append(candidate)
                seen_local.add(candidate.lower())
                remaining -= 1
                if remaining <= 0:
                    break

    remaining = width - len(filtered)
    if remaining > 0 and lw not in seen_local:
        filtered.append(word)

    seen_dedup = set()
    deduped = []
    for w in filtered:
        lw_check = w.lower()
        if lw_check not in seen_dedup:
            deduped.append(w)
            seen_dedup.add(lw_check)

    global_used.update(w.lower() for w in deduped)

    store_word_relations(word, deduped)
    return deduped[:width]


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Tree building
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _is_synthetic_word(word: str) -> bool:
    """
    Detect if a word is a synthetic variant that shouldn't breed further.
    """
    if len(word) < 3:
        return False

    lw = word.lower()

    if "_var" in lw or "_x" in lw:
        return True

    vowels = "aeiou–∞–µ—ë–∏–æ—É—ã—ç—é—è"
    vowel_count = sum(1 for c in lw if c in vowels)
    if vowel_count < max(1, len(lw) // 5):
        return True

    for i in range(len(lw) - 2):
        if lw[i] == lw[i+1] == lw[i+2]:
            return True

    if len(lw) >= 3 and lw[0] == lw[1] and lw[0].isalpha():
        return True

    if len(lw) >= 4:
        if lw.endswith("ss") and len(lw) > 4 and lw[-3] == lw[-4]:
            return True

    return False


def build_tree_for_word(
    word: str,
    width: int,
    depth: int,
    all_candidates: List[str],
    global_used: Optional[Set[str]] = None,
    is_core_word: bool = False
) -> Node:
    """
    Recursively mutate a word into a branching freak.
    """
    if global_used is None:
        global_used = set()

    node = Node(word=word)
    if depth <= 1:
        return node

    if _is_synthetic_word(word) and not is_core_word:
        return node

    first_level = lookup_branches_for_word(word, width, all_candidates, global_used)

    next_depth = depth - 1
    for b in first_level:
        child = build_tree_for_word(b, width=width, depth=next_depth, all_candidates=all_candidates, global_used=global_used, is_core_word=False)
        node.children.append(child)

    return node


def collect_leaves(node: Node) -> List[str]:
    """Gather all leaf words from the tree ‚Äî the final mutated fragments."""
    if not node.children:
        return [node.word]
    leaves: List[str] = []
    for ch in node.children:
        leaves.extend(collect_leaves(ch))
    return leaves


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Frankenstein reassembly ‚Äî markov-style corpse
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def reassemble_corpse(leaves: List[str]) -> str:
    """
    Build a new "sentence" from leaf words using a simple bigram chain.
    If leaves < 3, just shuffle and return.
    """
    if len(leaves) < 3:
        random.shuffle(leaves)
        return " ".join(leaves)

    bigrams: Dict[str, List[str]] = defaultdict(list)
    for i in range(len(leaves) - 1):
        bigrams[leaves[i].lower()].append(leaves[i + 1])

    current = random.choice(leaves)
    result = [current]
    seen = {current.lower()}

    target_len = random.randint(min(5, len(leaves)), min(10, len(leaves)))
    for _ in range(target_len):
        options = bigrams.get(current.lower(), [])
        options = [w for w in options if w.lower() not in seen]

        if not options:
            unused = [w for w in leaves if w.lower() not in seen]
            if not unused:
                break
            current = random.choice(unused)
        else:
            current = random.choice(options)

        result.append(current)
        seen.add(current.lower())

    return " ".join(result)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ASCII rendering ‚Äî vertical scalpel
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def render_node(node: Node, prefix: str, is_last: bool) -> List[str]:
    """Render one node and its descendants as a clean morgue diagram."""
    connector = "‚îî‚îÄ " if is_last else "‚îú‚îÄ "
    lines = [f"{prefix}{connector}{node.word}"]

    if node.children:
        new_prefix = prefix + ("   " if is_last else "‚îÇ  ")
        for i, ch in enumerate(node.children):
            last = (i == len(node.children) - 1)
            lines.extend(render_node(ch, new_prefix, last))

    return lines


def render_autopsy(prompt: str, words: List[str], trees: List[Node]) -> str:
    """Stitch together the full autopsy report as a single text block."""
    out: List[str] = []
    out.append(prompt.strip())
    out.append("")

    for w, t in zip(words, trees):
        out.append(w)
        for i, ch in enumerate(t.children):
            last = (i == len(t.children) - 1)
            out.extend(render_node(ch, "  ", last))
        out.append("")

    all_leaves: List[str] = []
    for t in trees:
        all_leaves.extend(collect_leaves(t))

    if all_leaves:
        corpse = reassemble_corpse(all_leaves)
        out.append("AUTOPSY RESULT:")
        out.append(f"  {corpse}")
        out.append("")

    out.append("‚Äî Sorokin")
    return "\n".join(out)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Main pipeline
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def sorokin_autopsy(prompt: str) -> str:
    """Main entry: take a prompt, return its dissection."""
    short = prompt.strip()[:MAX_INPUT_CHARS]
    tokens = tokenize(short)
    if not tokens:
        return "Nothing to dissect.\n\n‚Äî Sorokin"

    core = select_core_words(tokens)

    k = max(1, min(len(core), MAX_DEPTH))
    width = k
    depth = k

    all_candidates = tokens.copy()

    global_used: Set[str] = {w.lower() for w in core}

    trees = [build_tree_for_word(w, width, depth, all_candidates, global_used, is_core_word=True) for w in core]
    report = render_autopsy(short, core, trees)
    store_autopsy(short, report)
    return report


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# BOOTSTRAP EXTENSION ‚Äî Self-improving autopsy ritual
# Pattern accumulation without intelligence
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def _extract_mutation_paths(tree_text: str) -> List[Tuple[str, str, int]]:
    """Parse tree ASCII art to extract (source, target, depth) tuples."""
    paths: List[Tuple[str, str, int]] = []
    lines = tree_text.split('\n')
    
    parent_stack: List[Tuple[int, str]] = []
    
    for line in lines:
        if not line.strip():
            continue
        
        stripped = line.lstrip()
        if not stripped:
            continue
            
        indent = len(line) - len(stripped)
        depth = indent // 2
        
        word_match = re.search(r'[‚îî‚îú]‚îÄ\s*(\S+)', line)
        if not word_match:
            if depth == 0 and stripped and not stripped.startswith('‚îÄ'):
                word = stripped.strip()
                if word and not word.startswith('AUTOPSY') and word != '‚Äî':
                    parent_stack = [(depth, word)]
            continue
        
        word = word_match.group(1).strip()
        if not word:
            continue
        
        while parent_stack and parent_stack[-1][0] >= depth:
            parent_stack.pop()
        
        if parent_stack:
            parent_word = parent_stack[-1][1]
            paths.append((parent_word, word, depth))
        
        parent_stack.append((depth, word))
    
    return paths


def harvest_autopsy_patterns(autopsy_id: int, tree_text: str, corpse: str) -> None:
    """
    Extract successful mutation patterns from completed autopsy.
    This is the core corpus-building mechanism.
    """
    conn = sqlite3.connect(DB_PATH)
    try:
        paths = _extract_mutation_paths(tree_text)
        
        for source, target, depth in paths:
            existing = conn.execute(
                "SELECT id, total_count FROM mutation_templates WHERE source_word = ? AND target_word = ?",
                (source.lower(), target.lower())
            ).fetchone()
            
            if existing:
                conn.execute(
                    """UPDATE mutation_templates 
                       SET total_count = total_count + 1,
                           success_count = success_count + 1,
                           last_used = strftime('%s','now')
                       WHERE id = ?""",
                    (existing[0],)
                )
            else:
                conn.execute(
                    """INSERT INTO mutation_templates 
                       (source_word, target_word, path_depth, success_count, total_count)
                       VALUES (?, ?, ?, 1, 1)""",
                    (source.lower(), target.lower(), depth)
                )
        
        corpse_words = corpse.strip().split()
        for i in range(len(corpse_words) - 1):
            word1 = corpse_words[i].lower()
            word2 = corpse_words[i + 1].lower()
            
            existing = conn.execute(
                "SELECT id, frequency FROM corpse_bigrams WHERE word1 = ? AND word2 = ?",
                (word1, word2)
            ).fetchone()
            
            if existing:
                conn.execute(
                    "UPDATE corpse_bigrams SET frequency = frequency + 1 WHERE id = ?",
                    (existing[0],)
                )
            else:
                conn.execute(
                    "INSERT INTO corpse_bigrams (word1, word2, frequency) VALUES (?, ?, 1)",
                    (word1, word2)
                )
        
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()


def lookup_branches_bootstrap(
    word: str,
    width: int,
    all_candidates: List[str],
    global_used: Optional[Set[str]] = None
) -> List[str]:
    """
    Enhanced lookup using learned mutation templates.
    Priority order:
    1. mutation_templates (highest success_count)
    2. Original lookup (memory + phonetic + web)
    """
    width = max(1, width)
    if global_used is None:
        global_used = set()
    
    conn = sqlite3.connect(DB_PATH)
    try:
        rows = conn.execute(
            """SELECT target_word FROM mutation_templates 
               WHERE source_word = ? 
               ORDER BY success_count DESC, resonance_score DESC
               LIMIT ?""",
            (word.lower(), width * 2)
        ).fetchall()
    finally:
        conn.close()
    
    template_results = []
    for row in rows:
        target = row[0]
        if target.lower() not in global_used:
            template_results.append(target)
            if len(template_results) >= width:
                break
    
    if len(template_results) >= width:
        result = template_results[:width]
        global_used.update(w.lower() for w in result)
        return result
    
    remaining = width - len(template_results)
    original_results = lookup_branches_for_word(word, remaining, all_candidates, global_used)
    
    result = template_results + original_results
    return result[:width]


def reassemble_corpse_bootstrap(leaves: List[str]) -> str:
    """
    Enhanced reassembly using:
    - SEED_BIGRAMS (structural patterns from corpus)
    - corpse_bigrams (learned successful chains)
    - phonetic chaos (unpredictability)
    
    Weighted selection based on frequency, but NOT optimization.
    This is ritual repetition, not intelligence.
    """
    if len(leaves) < 3:
        random.shuffle(leaves)
        return " ".join(leaves)
    
    conn = sqlite3.connect(DB_PATH)
    try:
        rows = conn.execute(
            "SELECT word1, word2, frequency FROM corpse_bigrams"
        ).fetchall()
    finally:
        conn.close()
    
    weighted_bigrams: Dict[str, List[Tuple[str, int]]] = defaultdict(list)
    
    for word1, word2, freq in rows:
        weighted_bigrams[word1].append((word2, freq * 3))
    
    for word1, nexts in SEED_BIGRAMS.items():
        for word2 in nexts:
            weighted_bigrams[word1].append((word2, 2))
    
    for i in range(len(leaves) - 1):
        weighted_bigrams[leaves[i].lower()].append((leaves[i + 1], 1))
    
    current = random.choice(leaves)
    result = [current]
    seen = {current.lower()}
    
    target_len = random.randint(min(5, len(leaves)), min(10, len(leaves)))
    for _ in range(target_len):
        options = weighted_bigrams.get(current.lower(), [])
        
        valid_options = [(w, wt) for w, wt in options if w.lower() not in seen]
        
        if valid_options:
            words, weights = zip(*valid_options)
            chaos_weights = [w ** 0.5 for w in weights]
            current = random.choices(words, weights=chaos_weights, k=1)[0]
        else:
            unused = [w for w in leaves if w.lower() not in seen]
            if not unused:
                break
            current = random.choice(unused)
        
        result.append(current)
        seen.add(current.lower())
    
    return " ".join(result)


def compute_autopsy_resonance(tree_text: str, corpse: str, original_prompt: str) -> Dict[str, float]:
    """
    Compute resonance score based on:
    - Phonetic diversity (unique phonetic fingerprints / total words)
    - Semantic coherence (bigram overlap with known corpus / total bigrams)
    - Syntactic flow (inverse of word length variance)
    
    Pure structural metrics. NO embeddings. NO semantics.
    """
    corpse_words = corpse.strip().split()
    if not corpse_words:
        return {
            'phonetic_diversity': 0.0,
            'semantic_coherence': 0.0,
            'syntactic_flow': 0.0,
            'overall_resonance': 0.0
        }
    
    fingerprints = set()
    for word in corpse_words:
        fp = phonetic_fingerprint(word)
        if fp:
            fingerprints.add(fp)
    phonetic_diversity = len(fingerprints) / len(corpse_words) if corpse_words else 0.0
    
    corpse_bigrams = set()
    for i in range(len(corpse_words) - 1):
        corpse_bigrams.add((corpse_words[i].lower(), corpse_words[i+1].lower()))
    
    seed_bigram_set = set()
    for word1, nexts in SEED_BIGRAMS.items():
        for word2 in nexts:
            seed_bigram_set.add((word1, word2))
    
    if corpse_bigrams:
        overlap = len(corpse_bigrams & seed_bigram_set)
        semantic_coherence = overlap / len(corpse_bigrams)
    else:
        semantic_coherence = 0.0
    
    lengths = [len(w) for w in corpse_words]
    if len(lengths) > 1:
        mean_len = sum(lengths) / len(lengths)
        variance = sum((l - mean_len) ** 2 for l in lengths) / len(lengths)
        syntactic_flow = 1.0 / (1.0 + variance)
    else:
        syntactic_flow = 1.0
    
    overall = 0.4 * phonetic_diversity + 0.3 * semantic_coherence + 0.3 * syntactic_flow
    
    return {
        'phonetic_diversity': phonetic_diversity,
        'semantic_coherence': semantic_coherence,
        'syntactic_flow': syntactic_flow,
        'overall_resonance': overall
    }


def build_tree_for_word_bootstrap(
    word: str,
    width: int,
    depth: int,
    all_candidates: List[str],
    global_used: Optional[Set[str]] = None,
    is_core_word: bool = False
) -> Node:
    """Bootstrap version using lookup_branches_bootstrap()."""
    if global_used is None:
        global_used = set()

    node = Node(word=word)
    if depth <= 1:
        return node

    if _is_synthetic_word(word) and not is_core_word:
        return node
    
    first_level = lookup_branches_bootstrap(word, width, all_candidates, global_used)

    next_depth = depth - 1
    for b in first_level:
        child = build_tree_for_word_bootstrap(b, width=width, depth=next_depth,
                                              all_candidates=all_candidates, global_used=global_used, is_core_word=False)
        node.children.append(child)
    
    return node


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PATCH: Paragraph generation with syntax templates
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def fill_template(template: str, leaves: List[str]) -> str:
    """
    Fill template slots with words from leaves based on POS.
    Example: "{noun1} {verb} {noun2}" -> "darkness consumes reality"
    """
    # Build POS buckets
    pos_buckets = defaultdict(list)
    for w in leaves:
        pos = guess_pos(w)
        pos_buckets[pos].append(w)
    
    # Also add 'unknown' words to all buckets as fallback
    for w in leaves:
        if guess_pos(w) == 'unknown':
            for bucket in pos_buckets.values():
                if w not in bucket:
                    bucket.append(w)
    
    # Extract slots from template
    slots = re.findall(r'\{(\w+)\}', template)
    filled = template
    
    for slot in slots:
        # Extract POS type (e.g., "noun1" -> "noun")
        pos_type = re.sub(r'\d+', '', slot)
        
        if pos_buckets[pos_type]:
            word = random.choice(pos_buckets[pos_type])
            filled = filled.replace(f'{{{slot}}}', word, 1)
        elif pos_buckets['unknown']:
            # Fallback to unknown words
            word = random.choice(pos_buckets['unknown'])
            filled = filled.replace(f'{{{slot}}}', word, 1)
        else:
            # Last resort: any leaf
            word = random.choice(leaves)
            filled = filled.replace(f'{{{slot}}}', word, 1)
    
    return filled


def score_paragraph_resonance(paragraph: str) -> float:
    """
    Score paragraph quality based on:
    - Phonetic diversity
    - Rhythmic variance (syllable distribution)
    - Chaos factor (unpredictability bonus)
    
    Returns float 0.0-1.0
    """
    words = paragraph.strip().split()
    if not words:
        return 0.0
    
    # 1. Phonetic diversity
    fingerprints = {phonetic_fingerprint(w) for w in words}
    phon_div = len(fingerprints) / len(words)
    
    # 2. Rhythmic variance (syllable count variance)
    syllables = [count_syllables(w) for w in words]
    if len(syllables) > 1:
        mean_syl = sum(syllables) / len(syllables)
        variance = sum((s - mean_syl) ** 2 for s in syllables) / len(syllables)
        rhythm = min(variance / 2.0, 1.0)  # Cap at 1.0
    else:
        rhythm = 0.0
    
    # 3. Chaos factor (random, adds unpredictability)
    chaos = random.uniform(0.3, 0.7)
    
    return phon_div * 0.4 + rhythm * 0.4 + chaos * 0.2


def generate_sorokin_paragraph(leaves: List[str], n_sentences: int = 3) -> str:
    """
    Generate a multi-sentence paragraph with:
    - Syntactic templates (Sorokin-style)
    - POS-based slot filling
    - Punctuation (commas, periods)
    - Maximum resonance selection (generate 20 candidates, pick best)
    
    Returns a grammatically valid but semantically absurd paragraph.
    """
    if not leaves:
        return ""
    
    # Generate multiple candidates
    candidates = []
    for _ in range(20):
        sentences = []
        for _ in range(n_sentences):
            template = random.choice(SOROKIN_SENTENCE_TEMPLATES)
            sentence = fill_template(template, leaves)
            sentences.append(sentence)
        
        paragraph = ' '.join(sentences)
        score = score_paragraph_resonance(paragraph)
        candidates.append((score, paragraph))
    
    # Return best scoring paragraph
    candidates.sort(reverse=True, key=lambda x: x[0])
    return candidates[0][1]


def render_autopsy_bootstrap(prompt: str, words: List[str], trees: List[Node], 
                             resonance: Dict[str, float], stats: Dict[str, int]) -> str:
    """
    Enhanced visualization showing:
    - Original tree structure
    - AUTOPSY RESULT as Sorokin-style paragraph (PATCH: replaced simple corpse)
    - Resonance metrics (as ASCII progress bars)
    - Memory accumulation stats
    """
    out: List[str] = []
    out.append(prompt.strip())
    out.append("")
    
    for w, t in zip(words, trees):
        out.append(w)
        for i, ch in enumerate(t.children):
            last = (i == len(t.children) - 1)
            out.extend(render_node(ch, "  ", last))
        out.append("")
    
    all_leaves: List[str] = []
    for t in trees:
        all_leaves.extend(collect_leaves(t))
    
    # PATCH: Use paragraph generator instead of simple reassembly
    if all_leaves:
        paragraph = generate_sorokin_paragraph(all_leaves, n_sentences=random.randint(2, 4))
        out.append("AUTOPSY RESULT:")
        out.append(f"  {paragraph}")
        out.append("")
    
    def _render_bar(value: float, width: int = 10) -> str:
        """Render ASCII progress bar."""
        filled = int(value * width)
        return "‚ñà" * filled + "‚ñë" * (width - filled)
    
    out.append("RESONANCE METRICS:")
    out.append(f"  Phonetic Diversity: {_render_bar(resonance['phonetic_diversity'])} {resonance['phonetic_diversity']:.3f}")
    out.append(f"  Structural Echo:    {_render_bar(resonance['semantic_coherence'])} {resonance['semantic_coherence']:.3f}")
    out.append(f"  Mutation Depth:     {_render_bar(resonance['syntactic_flow'])} {resonance['syntactic_flow']:.3f}")
    out.append("")
    
    out.append("MEMORY ACCUMULATION:")
    out.append(f"  Known mutations: {stats['mutations']:,}")
    out.append(f"  Learned bigrams: {stats['bigrams']:,}")
    out.append(f"  Total autopsies: {stats['autopsies']:,}")
    out.append("")
    
    out.append("‚Äî Sorokin")
    return "\n".join(out)


def sorokin_autopsy_bootstrap(prompt: str) -> str:
    """
    Bootstrap-enhanced autopsy with pattern learning.
    Calls harvest_autopsy_patterns() after each run.
    """
    short = prompt.strip()[:MAX_INPUT_CHARS]
    tokens = tokenize(short)
    if not tokens:
        return "Nothing to dissect.\n\n‚Äî Sorokin"
    
    core = select_core_words(tokens)
    
    k = max(1, min(len(core), MAX_DEPTH))
    width = k
    depth = k
    
    all_candidates = tokens.copy()
    
    global_used: Set[str] = {w.lower() for w in core}
    
    trees = [build_tree_for_word_bootstrap(w, width, depth, all_candidates, global_used, is_core_word=True) for w in core]
    
    all_leaves: List[str] = []
    for t in trees:
        all_leaves.extend(collect_leaves(t))
    
    # PATCH: Generate paragraph instead of simple corpse
    corpse = generate_sorokin_paragraph(all_leaves, n_sentences=random.randint(2, 4)) if all_leaves else ""
    
    basic_report = render_autopsy(short, core, trees)
    
    conn = sqlite3.connect(DB_PATH)
    try:
        cursor = conn.execute(
            "INSERT INTO autopsy(prompt, tree_text) VALUES (?, ?)",
            (short, basic_report)
        )
        autopsy_id = cursor.lastrowid
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()
    
    harvest_autopsy_patterns(autopsy_id, basic_report, corpse)
    
    resonance = compute_autopsy_resonance(basic_report, corpse, short)
    
    conn = sqlite3.connect(DB_PATH)
    try:
        conn.execute(
            """INSERT INTO autopsy_metrics 
               (autopsy_id, phonetic_diversity, semantic_coherence, syntactic_flow, overall_resonance)
               VALUES (?, ?, ?, ?, ?)""",
            (autopsy_id, resonance['phonetic_diversity'], resonance['semantic_coherence'],
             resonance['syntactic_flow'], resonance['overall_resonance'])
        )
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()
    
    conn = sqlite3.connect(DB_PATH)
    try:
        mutations_count = conn.execute("SELECT COUNT(*) FROM mutation_templates").fetchone()[0]
        bigrams_count = conn.execute("SELECT COUNT(*) FROM corpse_bigrams").fetchone()[0]
        autopsies_count = conn.execute("SELECT COUNT(*) FROM autopsy").fetchone()[0]
    finally:
        conn.close()
    
    stats = {
        'mutations': mutations_count,
        'bigrams': bigrams_count,
        'autopsies': autopsies_count
    }
    
    return render_autopsy_bootstrap(short, core, trees, resonance, stats)


def repl(use_bootstrap: bool = False) -> None:
    """Endless dissection loop until the operator gives up. Bootstrap optional."""
    mode = "BOOTSTRAP" if use_bootstrap else "standard"
    print(f"SÃ¥Ã•ÃîoÃ¥ÕéÃørÃ∂ÃòÃíoÃ∏Ã∫ÃΩkÃµÃªÃàÃÅiÃ∑ÕñÕù√±Ã∂Õï online ({mode} mode). Type a prompt.")
    while True:
        try:
            prompt = input("> ").strip()
            if not prompt:
                continue
            if use_bootstrap:
                print(sorokin_autopsy_bootstrap(prompt))
            else:
                print(sorokin_autopsy(prompt))
            print()
        except (EOFError, KeyboardInterrupt):
            print("\nExiting autopsy room.")
            break


def main(argv: List[str]) -> None:
    init_db()
    if "--bootstrap" in argv:
        argv.remove("--bootstrap")
        use_bootstrap = True
    else:
        use_bootstrap = False
    
    if len(argv) > 1:
        prompt = " ".join(argv[1:])
        if use_bootstrap:
            print(sorokin_autopsy_bootstrap(prompt))
        else:
            print(sorokin_autopsy(prompt))
    else:
        repl(use_bootstrap=use_bootstrap)


if __name__ == "__main__":
    main(sys.argv)
```

---

## –ß—Ç–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å (PATCH sections):

1. **–°—Ç—Ä–æ–∫–∏ 88-97**: –î–æ–±–∞–≤–ª–µ–Ω `SOROKIN_SENTENCE_TEMPLATES` ‚Äî 8 —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö —à–∞–±–ª–æ–Ω–æ–≤ –≤ –¥—É—Ö–µ –°–æ—Ä–æ–∫–∏–Ω–∞

2. **–°—Ç—Ä–æ–∫–∏ 316-344**: –î–æ–±–∞–≤–ª–µ–Ω—ã:
   - `guess_pos()` ‚Äî —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π POS tagger
   - `count_syllables()` ‚Äî —Å—á—ë—Ç—á–∏–∫ —Å–ª–æ–≥–æ–≤ –¥–ª—è —Ä–∏—Ç–º–∞

3. **–°—Ç—Ä–æ–∫–∏ 1142-1233**: –î–æ–±–∞–≤–ª–µ–Ω—ã —Ñ—É–Ω–∫—Ü–∏–∏ paragraph generation:
   - `fill_template()` ‚Äî –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–ª–æ—Ç–æ–≤ —Å POS matching
   - `score_paragraph_resonance()` ‚Äî scoring (phonetic + rhythm + chaos)
   - `generate_sorokin_paragraph()` ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –≤—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –∏–∑ 20 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤

4. **–°—Ç—Ä–æ–∫–∞ 1253**: –í `render_autopsy_bootstrap()` –∑–∞–º–µ–Ω—ë–Ω –≤—ã–∑–æ–≤ `reassemble_corpse_bootstrap()` –Ω–∞ `generate_sorokin_paragraph()`

5. **–°—Ç—Ä–æ–∫–∞ 1312**: –í `sorokin_autopsy_bootstrap()` —Ç–æ–∂–µ –∑–∞–º–µ–Ω—ë–Ω –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä

---

## –†–µ–∑—É–ª—å—Ç–∞—Ç:

–¢–µ–ø–µ—Ä—å –≤–º–µ—Å—Ç–æ:
```
AUTOPSY RESULT:
  connection economise shelter greece representationalism literalism faithfulness
```

–ë—É–¥–µ—Ç:
```
AUTOPSY RESULT:
  The connection economises shelter in Greece, where representationalism becomes faithful. Literalism forgets corporeality. Nothing remains.
```

–ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–µ–∑—É–º–Ω–æ. –° —Ç–æ—á–∫–∞–º–∏, –∑–∞–ø—è—Ç—ã–º–∏, –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Ä–µ–∑–æ–Ω–∞–Ω—Å–Ω–æ–π –ª–æ–≥–∏–∫–æ–π üî™

–¢–µ—Å—Ç–∏—Ä—É–π –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ!


----–ò–î–ï–Ø –ü–†–û –†–ò–î–ú–ò:

–ö–õ–û–î:

–ë–ª—è, —Ç—ã **–ù–ï** –≥–æ–Ω–∏—à—å! –≠—Ç–æ –æ—Ö—É–µ–Ω–Ω–∞—è –∏–¥–µ—è!

README.md —É–∂–µ –µ—Å—Ç—å, –æ–Ω **–∂–∏–≤–æ–π** (–º–µ–Ω—è–µ—Ç—Å—è —Å –∫–∞–∂–¥—ã–º —Ä–∞—É–Ω–¥–æ–º), –∏ –æ–Ω –Ω–∞–ø–∏—Å–∞–Ω –≤ —ç—Ç–æ–º –ø—Å–∏—Ö–æ–ø–∞—Ç—Å–∫–æ–º –°–æ—Ä–æ–∫–∏–Ω-meets-Karpathy —Å—Ç–∏–ª–µ. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π seed corpus ‚Äî —ç—Ç–æ –∏–¥–µ–∞–ª—å–Ω–æ, –ø–æ—Ç–æ–º—É —á—Ç–æ:

1. **–ù–µ –Ω—É–∂–Ω–æ —Ö–∞—Ä–¥–∫–æ–¥–∏—Ç—å –Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç** ‚Äî README —É–∂–µ –µ—Å—Ç—å
2. **–≠–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç –≤–º–µ—Å—Ç–µ —Å –ø—Ä–æ–µ–∫—Ç–æ–º** ‚Äî –∫–∞–∂–¥—ã–π —Ä–∞–∑ –∫–æ–≥–¥–∞ —Ç—ã –æ–±–Ω–æ–≤–ª—è–µ—à—å README, corpus –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
3. **–°—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∏–¥–µ–∞–ª–µ–Ω** ‚Äî —Ç–∞–º —É–∂–µ –µ—Å—Ç—å —ç—Ç–∞ —Ä–µ–∑–æ–Ω–∞–Ω—Å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ ("ruthless autopsy", "morgue tables", "phonetic diversity")

---

## –ö–∞–∫ —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å:

### –í–∞—Ä–∏–∞–Ω—Ç 1: –ü–∞—Ä—Å–∏—Ç—å README.md –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ (–ª—ë–≥–∫–∏–π)

```python
def _build_readme_bigrams() -> Dict[str, List[str]]:
    """Extract structural bigrams from README.md if it exists."""
    readme_path = Path("README.md")
    if not readme_path.exists():
        return {}
    
    bigrams = defaultdict(list)
    try:
        text = readme_path.read_text(encoding='utf-8')
        # Remove code blocks (they're just examples)
        text = re.sub(r'```.*?```', ' ', text, flags=re.DOTALL)
        # Remove markdown headers
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)
        
        sentences = text.split('\n')
        for sentence in sentences:
            words = tokenize(sentence)
            for i in range(len(words) - 1):
                bigrams[words[i].lower()].append(words[i+1].lower())
    except Exception:
        pass  # If README parsing fails, just skip it
    
    return dict(bigrams)

README_BIGRAMS = _build_readme_bigrams()  # Loaded once at startup
```

–ü–æ—Ç–æ–º –≤ `reassemble_corpse_bootstrap()` –¥–æ–±–∞–≤–∏—Ç—å README bigrams —Å –≤–µ—Å–æ–º 1.5x (–º–µ–∂–¥—É seed corpus 2x –∏ local 1x):

```python
# Add README bigrams (medium-high weight)
for word1, nexts in README_BIGRAMS.items():
    for word2 in nexts:
        weighted_bigrams[word1].append((word2, int(1.5)))  # Weight: 1.5x
```

---

### –í–∞—Ä–∏–∞–Ω—Ç 2: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ (–µ—Å–ª–∏ README –º–µ–Ω—è–µ—Ç—Å—è –≤–æ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã)

–ï—Å–ª–∏ —Ç—ã —Ö–æ—á–µ—à—å —á—Ç–æ–±—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ README –ø—Ä–∏–º–µ–Ω—è–ª–∏—Å—å **–±–µ–∑ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞** —Å–∫—Ä–∏–ø—Ç–∞:

```python
def _load_readme_bigrams() -> Dict[str, List[str]]:
    """Dynamically load README bigrams (re-reads file each time)."""
    readme_path = Path("README.md")
    if not readme_path.exists():
        return {}
    
    bigrams = defaultdict(list)
    try:
        text = readme_path.read_text(encoding='utf-8')
        text = re.sub(r'```.*?```', ' ', text, flags=re.DOTALL)
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)
        
        sentences = text.split('\n')
        for sentence in sentences:
            words = tokenize(sentence)
            for i in range(len(words) - 1):
                bigrams[words[i].lower()].append(words[i+1].lower())
    except Exception:
        pass
    
    return dict(bigrams)


# –í reassemble_corpse_bootstrap():
def reassemble_corpse_bootstrap(leaves: List[str]) -> str:
    # ... existing code ...
    
    # PATCH: Add README bigrams (dynamically loaded)
    readme_bigrams = _load_readme_bigrams()
    for word1, nexts in readme_bigrams.items():
        for word2 in nexts:
            weighted_bigrams[word1].append((word2, int(1.5)))
    
    # ... rest of function ...
```

---

## –†–µ–∫–æ–º–µ–Ω–¥—É—é **–í–∞—Ä–∏–∞–Ω—Ç 1** (load once at startup):

- –ë—ã—Å—Ç—Ä–µ–µ (–Ω–µ –ø–∞—Ä—Å–∏—Ç README –∫–∞–∂–¥—ã–π —Ä–∞–∑)
- –ü—Ä–æ—â–µ (–æ–¥–∏–Ω –≤—ã–∑–æ–≤ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è)
- README –≤—Å—ë —Ä–∞–≤–Ω–æ –º–µ–Ω—è–µ—Ç—Å—è —Ä–µ–¥–∫–æ (—Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ —Ç—ã –µ–≥–æ –æ–±–Ω–æ–≤–ª—è–µ—à—å –≤—Ä—É—á–Ω—É—é)

–ï—Å–ª–∏ README –æ–±–Ω–æ–≤–∏–ª—Å—è ‚Äî –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏ —Å–∫—Ä–∏–ø—Ç. –≠—Ç–æ –Ω–æ—Ä–º –¥–ª—è workflow.

---

## –ü–æ–ª–Ω—ã–π –ø–∞—Ç—á (–¥–æ–±–∞–≤–∏—Ç—å –≤ sorokin.py):

```python
# –ü–æ—Å–ª–µ _build_seed_bigrams() –¥–æ–±–∞–≤–∏—Ç—å:

def _build_readme_bigrams() -> Dict[str, List[str]]:
    """Extract structural bigrams from README.md if it exists."""
    readme_path = Path("README.md")
    if not readme_path.exists():
        return {}
    
    bigrams = defaultdict(list)
    try:
        text = readme_path.read_text(encoding='utf-8')
        # Remove code blocks (just examples)
        text = re.sub(r'```.*?```', ' ', text, flags=re.DOTALL)
        # Remove markdown headers/formatting
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)
        text = re.sub(r'\*\*|\*|`', '', text)  # Remove bold/italic/code
        
        sentences = text.split('\n')
        for sentence in sentences:
            if not sentence.strip():
                continue
            words = tokenize(sentence)
            for i in range(len(words) - 1):
                bigrams[words[i].lower()].append(words[i+1].lower())
    except Exception:
        pass
    
    return dict(bigrams)

README_BIGRAMS = _build_readme_bigrams()  # Loaded once at startup
```

–ü–æ—Ç–æ–º –≤ `reassemble_corpse_bootstrap()` –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è SEED_BIGRAMS:

```python
# Add seed bigrams (weight: 2)
for word1, nexts in SEED_BIGRAMS.items():
    for word2 in nexts:
        weighted_bigrams[word1].append((word2, 2))

# PATCH: Add README bigrams (weight: 1.5)
for word1, nexts in README_BIGRAMS.items():
    for word2 in nexts:
        weighted_bigrams[word1].append((word2, int(1.5)))

# Add local leaf bigrams (weight: 1)
for i in range(len(leaves) - 1):
    weighted_bigrams[leaves[i].lower()].append((leaves[i + 1], 1))
```

---

–†–µ–∑—É–ª—å—Ç–∞—Ç: **–í–æ–ª–æ–¥—è —Ç–µ–ø–µ—Ä—å —É—á–∏—Ç—Å—è –Ω–µ —Ç–æ–ª—å–∫–æ –∏–∑ seed corpus, –Ω–æ –∏ –∏–∑ —Ç–≤–æ–µ–≥–æ README**, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–ø–∏—Å–∞–Ω –≤ —Å—Ç–∏–ª–µ "Sorokin –∏–∑–¥–µ–≤–∞–µ—Ç—Å—è –Ω–∞–¥ Karpathy". 

–ö–∞–∂–¥—ã–π —Ä–∞–∑ –∫–æ–≥–¥–∞ —Ç—ã –æ–±–Ω–æ–≤–∏—à—å README (–¥–æ–±–∞–≤–∏—à—å –Ω–æ–≤—ã–µ —Å–µ–∫—Ü–∏–∏, –ø—Ä–∏–º–µ—Ä—ã, –ø—Å–∏—Ö–æ–ø–∞—Ç—Å–∫–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏), —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç —Å—Ç–∞–Ω–µ—Ç —á–∞—Å—Ç—å—é —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –î–ù–ö –¥–ª—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏ —Ç—Ä—É–ø–æ–≤ üî™

–•–æ—á–µ—à—å —á—Ç–æ–±—ã —è –¥–æ–±–∞–≤–∏–ª —ç—Ç–æ –≤ –ø–æ–ª–Ω—ã–π —Ñ–∞–π–ª? –ò–ª–∏ —Å–∞–º –≤—Å—Ç–∞–≤–∏—à—å –∏ –ø–æ–π–¥—ë—à—å —Ç–µ—Å—Ç–∏—Ç—å?
