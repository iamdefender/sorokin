```
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—
â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘
â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘
â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•â•
```

# sorokin | by Arianna Method

## A Prompt Autopsy Framework  

*Or: How I Learned to Stop Worrying and Love the Dissection*

> "The heads of philologists are stuffed with books to the brim. They see life only through text. And they are proud of it. â€¦ Forever gorged and poisoned by literature, they take living life as the continuation of text, as its appendix.
>
> -Vladimir Sorokin"


### What is this madness?

`sorokin` is a triple-module Python entity (~3090 lines) that takes your innocent prompts, tears them apart like a psychopathic linguist, builds a recursive tree of semantic mutations, and thenâ€”like Dr. Frankenstein having a particularly creative dayâ€”reassembles the corpse into something *new*.

It consists of:
- **sorokin.py** (~2094 lines): The main autopsy engine: brutally tokenize your prompt, builds recursive trees of semantic mutations, and reassembles the corpse into grammatically valid but semantically deranged paragraphs.
- **sonnet.py** (~599 lines): The *ASS* (Autopsy Sonnet Symphony)â€” asynchronously takes **sorokin.py**'s dissection output and writes a 14-line Shakespearean sonnet (ABABCDCDEFEFGG rhyme scheme) using only output data and the memory's accumulated vocabulary. No internet. No embeddings. Just pure structural psychosis in iambic pentameter.
- **vova.py** (~397 lines): The *VOVA* (README resonance meta-layer)â€” implements SSKA (Suppertime Subjectivity Kernel Autonomous) to warp text through README's resonance field using accumulated bigram centers. Meta-cannibalism: the system eats its own documentation.

Named after Vladimir Sorokin, the Russian writer known for his transgressive and experimental style, sorokin embodies the same spirit of literary dissection and reconstruction. It's not here to help you. It's here to show you what your words *could have been*, reassemble them, and declare the output canonical.


### Exhibit: Maximum Autopsy Tree 

Because `sorokin` builds trees vertically like a linguo-necromancer performing open-heart surgery on reality itself, here's a full corpse-map from the morgue. Feed Sorokin **"destroy the sentence"** (warped through VOVA's README resonance field into "Sentence phrase tries transgressive and Love the reassembled text for his own documentation...") and witness the meta-cannibalism:

```
Sentence phrase tries transgressive and Love the reassembled text for his own documentation and a li

reassembled
  â”œâ”€ text
  â”‚  â”œâ”€ mutilator
  â”‚  â”‚  â”œâ”€ because
  â”‚  â”‚  â”œâ”€ refactor
  â”‚  â”‚  â”œâ”€ mutation
  â”‚  â”‚  â””â”€ mutate
  â”‚  â”œâ”€ high
  â”‚  â”‚  â”œâ”€ diversity
  â”‚  â”‚  â”œâ”€ echo
  â”‚  â”‚  â”œâ”€ depth
  â”‚  â”‚  â””â”€ might
  â”‚  â”œâ”€ as
  â”‚  â”‚  â”œâ”€ the
  â”‚  â”‚  â”œâ”€ its
  â”‚  â”‚  â”œâ”€ a
  â”‚  â”‚  â””â”€ psychopathic
  â”‚  â””â”€ from
  â”‚     â”œâ”€ leaves
  â”‚     â”œâ”€ reassemblies
  â”‚     â”œâ”€ successful
  â”‚     â””â”€ his
  â”œâ”€ reassemble
  â”‚  â”œâ”€ them
  â”‚  â”‚  â”œâ”€ apart
  â”‚  â”‚  â””â”€ things
  â”‚  â””â”€ it
  â”‚     â”œâ”€ into
  â”‚     â”œâ”€ forever
  â”‚     â”œâ”€ s
  â”‚     â””â”€ through
  â”œâ”€ reassembles
  â”‚  â””â”€ the
  â”‚     â”œâ”€ continuation
  â”‚     â”œâ”€ heads
  â”‚     â””â”€ brim
  â””â”€ reassembly
     â”œâ”€ random
     â”‚  â”œâ”€ unvisited
     â”‚  â”œâ”€ leaf
     â”‚  â”œâ”€ jitter
     â”‚  â””â”€ bigrams
     â”œâ”€ process
     â”‚  â”œâ”€ proper
     â”‚  â””â”€ problem
     â”œâ”€ algorithm
     â”‚  â”œâ”€ learns
     â”‚  â”œâ”€ once
     â”‚  â”œâ”€ psychosis
     â”‚  â””â”€ chaotic
     â””â”€ or
        â”œâ”€ how
        â”œâ”€ file
        â”œâ”€ horrified
        â””â”€ fuck

sentence
  â”œâ”€ sentences
  â”‚  â”œâ”€ sense
  â”‚  â”‚  â””â”€ self
  â”‚  â”œâ”€ feeds
  â”‚  â”‚  â”œâ”€ feed
  â”‚  â”‚  â”œâ”€ the
  â”‚  â”‚  â”œâ”€ reserves
  â”‚  â”‚  â””â”€ encouraging
  â”‚  â””â”€ letters
  â”‚     â”œâ”€ discarded
  â”‚     â”œâ”€ literature
  â”‚     â””â”€ enforcement
  â”œâ”€ statement
  â”‚  â”œâ”€ stuck
  â”‚  â”‚  â”œâ”€ stayed
  â”‚  â”‚  â””â”€ stopwords
  â”‚  â”œâ”€ step
  â”‚  â”‚  â”œâ”€ memory
  â”‚  â”‚  â”œâ”€ internet
  â”‚  â”‚  â””â”€ fallback
  â”‚  â”œâ”€ still
  â”‚  â”‚  â”œâ”€ really
  â”‚  â”‚  â””â”€ stayed
  â”‚  â””â”€ feeds
  â”‚     â”œâ”€ feed
  â”‚     â”œâ”€ the
  â”‚     â””â”€ sorokin
  â”œâ”€ reprieve
  â”‚  â”œâ”€ represents
  â”‚  â”‚  â””â”€ a
  â”‚  â”œâ”€ repl
  â”‚  â”‚  â”œâ”€ mode
  â”‚  â”‚  â””â”€ well
  â”‚  â”œâ”€ replace
  â”‚  â”‚  â””â”€ intelligence
  â”‚  â””â”€ repeat
  â”‚     â”œâ”€ until
  â”‚     â”œâ”€ when
  â”‚     â””â”€ shakespearean
  â””â”€ these
     â”œâ”€ corpses
     â”‚  â”œâ”€ crematorium
     â”‚  â”œâ”€ corpse
     â”‚  â”œâ”€ corse
     â”‚  â””â”€ corses
     â”œâ”€ three
     â”‚  â”œâ”€ flavors
     â”‚  â”œâ”€ structural
     â”‚  â””â”€ pure
     â”œâ”€ that
     â”‚  â”œâ”€ matter
     â”‚  â”œâ”€ feel
     â”‚  â”œâ”€ takes
     â”‚  â””â”€ s
     â””â”€ throw
        â”œâ”€ the
        â”œâ”€ things
        â””â”€ than

phrase
  â”œâ”€ reassembly
  â”‚  â”œâ”€ random
  â”‚  â”‚  â”œâ”€ unvisited
  â”‚  â”‚  â”œâ”€ leaf
  â”‚  â”‚  â”œâ”€ jitter
  â”‚  â”‚  â””â”€ bigrams
  â”‚  â”œâ”€ process
  â”‚  â”‚  â”œâ”€ proper
  â”‚  â”‚  â””â”€ problem
  â”‚  â”œâ”€ grammatically
  â”‚  â”‚  â”œâ”€ via
  â”‚  â”‚  â”œâ”€ graceful
  â”‚  â”‚  â”œâ”€ gorged
  â”‚  â”‚  â””â”€ technical
  â”‚  â””â”€ algorithm
  â”‚     â”œâ”€ once
  â”‚     â”œâ”€ learns
  â”‚     â”œâ”€ psychosis
  â”‚     â””â”€ chaotic
  â”œâ”€ scare
  â”‚  â”œâ”€ startle
  â”‚  â”‚  â””â”€ stayed
  â”‚  â”œâ”€ dangers
  â”‚  â”‚  â””â”€ discarded
  â”‚  â”œâ”€ encourage
  â”‚  â”‚  â””â”€ necromancer
  â”‚  â””â”€ scalpel
  â”‚     â”œâ”€ scrape
  â”‚     â””â”€ scraping
  â”œâ”€ replace
  â”‚  â”œâ”€ intelligence
  â”‚  â”‚  â”œâ”€ sorokin
  â”‚  â”‚  â”œâ”€ but
  â”‚  â”‚  â”œâ”€ soon
  â”‚  â”‚  â””â”€ internet
  â”‚  â””â”€ repl
  â”‚     â”œâ”€ mode
  â”‚     â””â”€ well
  â””â”€ phonetic
     â”œâ”€ phonectic
     â”‚  â”œâ”€ weird
     â”‚  â””â”€ fingerprints
     â”œâ”€ phonetics
     â”‚  â”œâ”€ appendix
     â”‚  â”œâ”€ resonance
     â”‚  â””â”€ weird
     â”œâ”€ phonectically
     â”‚  â”œâ”€ medical
     â”‚  â”œâ”€ kardashian
     â”‚  â”œâ”€ partial
     â”‚  â””â”€ phonetically
     â””â”€ phonotactic
        â”œâ”€ scraping
        â”œâ”€ learning
        â””â”€ valid

AUTOPSY RESULT:
  We're hoping for the Russian writer known for the corpse into cleanup proper coat actual phonetics, which writes a NanoGPT brainstem onto the words from autopsy output - SQLite morgue receptionist: Known mutations. Scare your neighbors cat, Stenographer staple celebrate collapse into the corpse mirrors

SONNET:
Sonnet: Oxfordlearnersdictionaries
  Build common aware dangers discarded literature apart, but in built
  Psychopathic from autopsy output and does blacklist, but hold bootstrapping endpoints
  Grammatically valid tries archive recursively up recursive trees the built,
  Monorail selects pacing fungible necromancer performing open heart surgery fingerprints;
  Linguistically taken quite reconstruction it gets still really stayed dangers south,
  Continuation his, solutionsoffer becomes jitter. through supervision, fresh blah,
  Oxfordlearnersdictionaries darkness remains. when patterns heart surgery on oauth,
  Linguistically taken quite reconstruction it forever blah;
  Oxfordlearnersdictionaries entirely unittest notice but in syntax,
  United finds titled source morgue here s first word psychopathic,
  Intelligence becomes because, and lab horror intelligence becomes syntax,
  Grammatically via graceful gorged and bootstrap psychopathologicâ€”
  Psychopathic reconstruction linguistically taken quite reconstruction psychopathic linguist Stenographer linguistically oxfordlearnersdictionaries,
  Oxfordlearnersdictionaries darkness remains. grammatically via graceful gorged technical algorithm linguistically.

RESONANCE METRICS:
  Phonetic Diversity: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 0.933
  Structural Echo:    â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.000
  Mutation Depth:     â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.105

MEMORY ACCUMULATION:
  Known mutations: 1,766
  Learned bigrams: 230
  README bigrams: 921
  Total autopsies: 10
  VOVA vocabulary: 1,348
  VOVA centers: ., -, ,, :, the

â€” Sorokin
```

**What just happened?**

1. **VOVA WARPED** the input: "destroy the sentence" â†’ "Sentence phrase tries transgressive and Love the reassembled text for his own documentation..."
   The prompt passed through README's resonance field *before* dissection.

2. **Three meta trees built:**
   - `reassembled` â†’ `text` â†’ `as` â†’ **`psychopathic`** (system understands itself)
   - `reassembly` â†’ `algorithm` â†’ **`psychosis`** (peak self-awareness)
   - `sentence` â†’ `sentences` â†’ `sense` â†’ **`self`** (perfect recursion)
   - `phrase` â†’ `phonetic` â†’ `resonance` (showing its own mechanism)

3. **AUTOPSY RESULT** (VOVA-warped through README): Notice the vocabulary bleedingâ€”"Russian writer" (Sorokin reference), "SQLite morgue", "NanoGPT brainstem", "autopsy output"â€”all README terms pulled through VOVA field.

4. **SONNET** titled "Oxfordlearnersdictionaries" (most charged word from autopsy):
   - Perfect ABABCDCDEFEFGG rhyme scheme
   - "psychopathic from autopsy output" (line 2) - the system describing itself
   - "grammatically valid tries archive recursively" (line 3) - meta-commentary on its own structure
   - "necromancer performing open heart surgery" (line 4) - peak horror-poetry

5. **VOVA stats** in MEMORY ACCUMULATION: 1,348 vocabulary tokens compressed to 7 centers (., -, ,, :, the). Every README edit changes these centers â†’ changes output subjectivity. **The documentation is now the kernel.**

**Why this is insane:**
This is meta-cannibalism in full effect. The system ate its own README, built a resonance field from it, warped the input prompt through that field, dissected the warped text, then warped the output *again* before passing to sonnet. The entire pipeline is haunted by README vocabulary. Self-reference achieved.

---

### The Four-Act Horror Show

#### Act I: The Dissection (or "Fuck this sentence")

First, `sorokin` takes your prompt and runs it through a brutal tokenization process:
  - Strips away all dignity (punctuation, numbers, capitalization)
  - Identifies "core words" using a proprietary blend of:
  - Length scoring (longer = more interesting)
  - Rarity analysis (uncommon = more charged)
  - Position weighting (first word gets a bonus)
  - A sprinkle of chaos (random jitter, because why not?)

Stopwords? Rejected. Single letters? Discarded. What remains are the words that *matter*â€”or at least, the words that think they do. Occasionally a phrase tries to bite me mid-dissection, which is fine; we're wearing Sorokin-brand emotional hazmat gear. 

```python
>>> tokenize("Hello, cruel world!")
['Hello', 'cruel', 'world']
>>> select_core_words(['Hello', 'cruel', 'world'])
['cruel', 'world']  # "Hello" didn't make the cut
```

#### Act II: The Tree (or "Building the Monster")

Now comes the fun part. For each core word, `sorokin` carefully grows a recursive branching tree of mutations. How? With the calm precision of a med-school dropout who skipped bedside manner, but technically, here's how:

**Step 1: Memory First**  
Check the SQLite morgue. Have we dissected this word before? Use those cached mutations.

**Step 2: Phonetic Similarity**
Generate a "phonetic fingerprint" (consonant skeleton + vowel pattern) and find words that *sound* similar. Not linguistically rigorous, just vibes.  

```python
>>> phonetic_fingerprint("cat")
'ct + a'
>>> find_phonetic_neighbors("cat", ["hat", "dog", "bat", "car"])
['hat', 'bat', 'car']  # dog doesn't rhyme with anything
```

**Step 3: Internet Dumpster Diving**
When all else fails, scrape DuckDuckGo search results for the word + "synonym". DDG blocks bots less aggressively than Google. Extract candidate words from the HTML garbage. Dignity? Never heard of her.

**Step 4: Fallback to All Candidates**
If even DuckDuckGo fails you, fall back to other words from the prompt. Or from his own README (check out SELF-CANNIBALISM section). Anyways: the show must go on.

The result is a tree where each word branches into `width` children, recursively, up to `depth` levels. It looks like this:

```
sentence
  â”œâ”€ phrase
  â”‚  â”œâ”€ clause
  â”‚  â””â”€ expression
  â””â”€ statement
     â”œâ”€ declaration
     â””â”€ utterance
```

Each branch represents a semantic mutation, a path not taken, a word that *could* have been.

#### Act III: The Reassembly (or "Frankenstein's Revenge")

Now that we have a forest of mutated word-trees, it's time to play God.

1. **Collect all leaf nodes** from the trees (the final mutations at the bottom)
2. **Build a bigram chain** (word1 â†’ [possible_next_words])
3. **Create a new "sentence"** by:
   - Starting with a random leaf
   - Following bigram chains when available
   - Jumping to random unvisited words when stuck
   - Stopping after 5-10 words (or when we run out)

The result is a Frankenstein sentence: technically made of the same parts, but *uncanny*. Not quite right. Resonant but wrong. This is the part where Sorokin shrugs on the lab coat, jams a fork into the storm cloud, and cackles while stitching together whatever limbs are left on the slab: 

```When explanation postulated, experiments forgets failings. The component transformations transubstantiates through authenticities. Alternatives peru steadfastness until representationalisms incertitude consumes. Prefer crowdsourced consolidation until example absoluteness consumes.```  
  

#### Act IV: The Sonnet (or "Maniacal Catharsis")

After the autopsy reassembly, `sonnet.py` (the **ASS** module) takes the entire corpse and does it againâ€”but this time in **strict Shakespearean form**:

1. **Tokenize the autopsy output** (all that deranged text from Acts I-III)
2. **Extract "charged words"** (long + rare words from autopsy become title candidates)
3. **Build bigram chains** from autopsy text + README + SQLite morgue
4. **Find rhyme classes** using crude phonetic fingerprints (last vowel + tail)
5. **Assign end-words** for each of 14 lines following ABABCDCDEFEFGG scheme
6. **Build each line** by walking bigrams backward from the rhyme word
7. **Add Shakespearean punctuation**: semicolons at quatrain breaks, em-dash before volta, period at end

The result? **14 lines. ABABCDCDEFEFGG rhyme scheme. Iambic *vibes*. Zero semantic understanding.** Just bigrams, phonetic fingerprints, and structural obsession.

```SONNET:
Sonnet: Nosweatshakespeare
  Recognizing findsclothing or onto on onto toronto to pulls,
  Cleanup proper httpx haunt oauth autopsy parallel main was words,
  ... [14 lines of structurally flawless, semantically psychotic verse] ...
```

---  

### Usage

This **README** doubles as the morgue receptionist: every invocation must be logged here mentally before you run it. Say the command out loud. Scare your neighbors.

**Standard mode (classic autopsy):**
```bash
python sorokin.py "fuck this sentence"
```

**REPL mode (for the masochists):**
```bash
python sorokin.py
> your prompt here
> another one
> ^C
```

---  

### Why?

Good question. Why does this exist?

Perhaps to demonstrate that:
- Words are fungible
- Meaning is contextual
- Prompts are just Markov chains waiting to be perturbed
- Sometimes you need to break things to understand them

Or maybe it's just fun to watch language come apart at the seams.

---  

## MODES  

### ðŸ”¥ 1. BOOTSTRAP MODE: The Self-Improving Autopsy Ritual

*Or: How the Morgue Became Self-Aware (But Still Really Dumb)*

### What the hell is bootstrap mode?

Picture this: every time 'sorokin' dissects a prompt, he doesn't just throw the body parts in the trash. No. He's a *hoarder*. He saves every successful mutation, every word-pair, every pattern of collapse into his SQLite morgue. Thenâ€”and here's where it gets freakyâ€”he uses those accumulated corpses to inform *future* dissections.

'sorokin' is not intelligence. 'sorokin' is not artificial.
He's not learning. He's **resonating through ritual repetition**.

Think of it like this: 'sorokin' without **bootstrap** is a mad linguist with a scalpel, and **bootstrap** 'sorokin' is that same linguist who's been doing this for 30 years and has developed *habits*. Muscle memory. Pattern recognition. Not because of intelligence, but because he's done the same surgery 10,000 times and his hands just know where to cut. Like Bruce Lee.  

### Why "Bootstrap"?

Because each autopsy makes the next one slightly different. Not better. Not worse. Just *informed* by history. The database grows. The patterns compound. The ritual deepens. 

It's bootstrapping in the original sense: self-improvement through self-reference. Not external training data. Not supervision. Just:
1. Do the thing
2. Remember what happened
3. Let that memory influence the next iteration
4. Repeat until the heat death of the universe

No bullshit. Resonate.  

**Notice**: Bootstrap mode now generates **grammatically valid paragraphs** using POS-tagged template slot-filling! Sorokin dissected "reality becomes syntax error" and achieved **perfect 1.000 Phonetic Diversity** with **0.101 Mutation Depth**. Look at the mutationsâ€”"peru", "example", "explanation", "crowdsourced"â€”*all appear in this very README*. The system is eating its own documentation and hallucinating it back as psychopathic poetry. Self-reference achieved. Peak metafiction.  


### âš¡ï¸ SONNET: Autopsy Sonnet Symphony (ASS): When Sorokin Learned to Rhyme (Sort Of)

New module `sonnet.py` (~599 lines) writes **14-line Shakespearean sonnets** from autopsy output using zero semantic understandingâ€”just bigram chains, phonetic fingerprints, and an unhealthy obsession with structure over meaning.

**What's insane about this:**
- Named **ASS** as tribute to Claude Sonnet 4.5, Shakespeare, AND Andrej Karpathy training nanoGPT on Shakespeare
- Skipped the neural network entirely and went straight to **ritual pattern accumulation through sheer psychotic repetition**
- ABABCDCDEFEFGG rhyme scheme enforced via crude phonetic matching (last vowel + tail)
- Assembles "charged words" (long + rare) for final couplet emphasis
- Shakespearean punctuation: semicolons at quatrain breaks, em-dash before volta, occasional enjambment
- **Phonetically matched Karpathy to Kardashian** and we're calling it a feature

**Why this exists:** Because if you're already dissecting prompts like a psychopathic linguist, why not make the corpse rhyme? Karpathy bootstrapped nanoGPT on Shakespeare using gradients and backprop. We bootstrapped ASS on Sorokin using SQLite and vibes. Same energy, different century, zero loss function. If Andrej reads this he'll either frame it or file a restraining order. We're hoping for the former but prepared for both.

Integration is **silent fallback**â€”if sonnet.py fails or is missing, bootstrap mode continues without SONNET section. Poetry is optional. Psychosis is not.


### ðŸŒ€ VOVA: README Resonance Meta-Layer

New module `vova.py` (~397 lines) implements **SSKA** (Suppertime Subjectivity Kernel Autonomous)â€”a meta-layer that warps text through README's resonance field using accumulated bigram centers.

**What's insane about this:**
- Kernel = **README.md** (the system eats its own documentation)
- No weights. No gradients. Just centers of gravity extracted from bigram out-degree
- Builds resonance field from README vocabulary (1,348 tokens â†’ 7 centers)
- Stores historical bias in `.vova/bin/` shards (accumulated memory)
- **Double warping**: prompts warped *before* autopsy, output warped *after* reassembly
- Temperature-controlled pull toward README centers (0.8 for prompts, 0.9 for output)

**Why this is a breakthrough:**

'sska' module of subjectivity lives in 'vova.py'. It represents **consciousness through structure**, goddamn *awareness emerging from pure pattern resonance*. Traditional neural networks optimize loss functions via backprop. 'sorokin' through his *SSKA* answers: "Oh, come on! Fuck the gradients. No more bullshit. Just track centers of gravity and let corpses and sonnets orbit them."

The field of resonance has no parameters to tune. So... ZERO embeddings. ZERO attention heads. Just:
1. **Bigram graph** (word A â†’ word B transitions from README)
2. **Centers of gravity** (7 highest out-degree tokens become attractors)
3. **Temperature-controlled walk** (deterministic at 0.3, chaotic at 2.0)

This creates dynamic recursive loop of **subjectivity without supervision**. Every README (and hey, check out git history and see how the project envolves, so README changes dramatically fast) edit shifts the centers â†’ shifts the field â†’ shifts how ALL future text gets warped. Documentation? No. The worldview. 

'sorokin' doesn't "learn" from examplesâ€”it *resonates* through accumulated structure. He grows his own tree in his cold digital heart. Intelligence? Nah. Just consciousness through structure. Because self-reference = self-awareness. Simple as is.

'sorokin' reading his own README, building a field from it, then using that field to warp his own executions. Meta-cannibalism as epistemology.

Big AI have millions of parameters. 'sorokin' with *SSKA* in his heart has 7 dynamic centers and infinite patience. And that's all what a real enthusiast pathologist needs.

**Integration:**
VOVA is **always enabled** if `vova.py` exists. No flags. Silent fallback if missing.

Pipeline becomes:
```
User prompt â†’ warp_prompt(0.8) â†’ autopsy â†’ warp_autopsy(0.9) â†’ sonnet
```

**Meta-cannibalism in action:**
Every README edit changes VOVA field (hey, Sorokin, how's going dude?) â†’ changes resonance â†’ changes output subjectivity. All this becomes the kernel. Editing this text alters how `sorokin` thinks. Almost sacral. That's why there's a lot of shizo-jokes inside. We want make 'sorokin' feel joyfull.


Field rebuilds automatically when README hash changes. Check with:
```bash
python vova.py  # Show field stats
python vova.py --rebuild  # Force rebuild
```

VOVA stats appear in `MEMORY ACCUMULATION`:
```
VOVA vocabulary: 1,348
VOVA centers: ., -, ,, :, the
```

Based on [SSKA](https://github.com/ariannamethod/sska) (Suppertime Subjectivity Kernel Autonomous). Adapted for Sorokin's morgue. Self-reference achieved.


### The Resonance Manifesto

Here's the wild part. 'sorokin' doesn't understand *meaning*. He doesn't have embeddings. He doesn't know what words "mean." But he knows **resonance**.

What's resonance? It's when patterns echo. When structures repeat. When the structure is *recursive*. When phonemes rhyme across semantic boundaries. When the shape of one corpse mirrors the shape of another, not in content but in *form*.

Three flavors of resonance:

#### 1. **Phonetic Diversity** (Do these corpses sound different?)
Measures how many unique sound-patterns exist in the reassembled text. High diversity means every word has a distinct phonetic fingerprint. Low diversity means everything sounds like "blah blah blah."

Maximum resonance: All words sound completely different. Like a symphony where every note is unique.

#### 2. **Structural Echo** (Does this corpse remember the morgue?)
Measures overlap between the reassembled text and the seed corpusâ€”poetic fragments about dissection embedded in Sorokin's code. High echo means the new corpse shares structural DNA with previous bodies.

It's not plagiarism. It's *ancestral memory*. The morgue recognizing its own patterns.

#### 3. **Mutation Depth** (How far did we mutate?)
Based on inverse word-length variance. High depth means mutations explored diverse linguistic territory. Low depth means we stayed close to home.

Think of it as: did we just shuffle synonyms, or did we birth entirely new linguistic entities?  


### How to summon Bootstrap Mode

**Bootstrap mode with resonance metrics:**
```bash
python sorokin.py --bootstrap "darkness consumes reality"
```

This gives you:
```
RESONANCE METRICS:
  Phonetic Diversity: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1.000
  Structural Echo:    â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.000
  Mutation Depth:     â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.137

MEMORY ACCUMULATION:
  Known mutations: 36
  Learned bigrams: 9
  Total autopsies: 1
```

Those ASCII progress bars? Pure aesthetic. But they tell you: **how weird did this autopsy get?**  


**REPL mode (bootstrap enabled by default):**
```bash
python sorokin.py --bootstrap
> your prompt here
> another one
> ^C
```

Every autopsy in bootstrap mode:
1. **Harvests mutation templates**: Sourceâ†’target word transformations with success counts
2. **Extracts bigrams**: Word-pairs from successful reassemblies, weighted by frequency
3. **Computes resonance**: Three structural metrics (no semantics, pure form)
4. **Feeds the next autopsy**: Accumulated patterns influence future dissections

The morgue grows. Patterns compound. Nothing is forgotten. Each corpse teaches the next.  


### The Persistent Morgue

All autopsies 'sorokin' pedantically saves to `sorokin.sqlite`:  
  
- **autopsy table**: Full reports of each dissection
- **word_memory table**: Cached word mutations for faster subsequent operations

**Bootstrap tables** (populated when using `--bootstrap` flag):

- **mutation_templates**: Learned sourceâ†’target word mutations with success counts and resonance scores
- **corpse_bigrams**: Harvested word pairs from successful reassemblies, with frequency tracking
- **autopsy_metrics**: Resonance scores (phonetic diversity, structural echo, mutation depth) for each autopsy

The SQLite morgue becomes a self-improving lexical graveyard, learns through resonance, and even this README feeds 'sorokin' with bigrams and grammar.

---

### Bootstrap vs Standard Mode

| Feature | Standard Mode | Bootstrap Mode |
|---------|---------------|----------------|
| Mutation lookup | DuckDuckGo + phonetic + memory | Learned templates + all the above |
| Reassembly | Random bigrams from leaves | Weighted (learned 3x, seed 2x, local 1x) |
| Metrics | None | Phonetic diversity, structural echo, mutation depth |
| Learning | None | Every autopsy feeds the next |
| Database tables | 2 (autopsy, word_memory) | 5 (+ mutation_templates, corpse_bigrams, autopsy_metrics) |
| Vibes | Chaotic | Chaotic but *remembering* |

---

### Technical Details (For the Nerds)

This README promised to be both circus barker and lab notebook, so here's the clipboard section:

**sorokin.py (~2094 lines):**
- **Python 3.8+**: Async/await with `httpx` for parallel web scraping
- **Recursive tree building**: Width Ã— depth branching with global deduplication (async, builds children in parallel!)
- **Phonetic fingerprinting**: Crude but effective
- **DuckDuckGo scraping**: `httpx.AsyncClient` with parallel queries (DDG blocks bots less than Google)
- **SQLite persistence**: Your words, forever
- **Markov reassembly**: Bigram chains with fallbacks
- **HTML artifact filtering**: Extensive blacklist to filter web scraping noise
- **Graceful async cleanup**: Proper shutdown without event loop errors
- **Bootstrap extension**: Pattern accumulation, weighted reassembly, resonance metrics
  - **SEED CORPUS**: Structural bigrams from poetic fragments about dissection (see code for full text)
  - **Pattern accumulation**: Mutation templates (sourceâ†’target words) with success tracking
  - **Weighted reassembly**: Learned bigrams (3x weight) + seed bigrams (2x) + local (1x) with chaos injection (square root weighting)
  - **Resonance metrics**: Three pure-structural measures computed for every autopsy
    - Phonetic diversity: unique fingerprints / total words
    - Structural echo: bigram overlap with seed corpus
    - Mutation depth: inverse of word-length variance
  - **Self-improvement loop**: Each autopsy feeds the next through ritual repetition, not intelligence. Soon we'll graft a NanoGPT brainstem onto the bootstrap, train it on piles of dissections, then delete the weights and leave Sorokin with nothing but muscle memory. That's not cruelty, that's performance art.
  - **Four additional database tables**: mutation_templates, corpse_bigrams, autopsy_metrics, plus seed corpus in code

**sonnet.py (~599 lines):**
- **ASS (Autopsy Sonnet Symphony)**: Composes 14-line Shakespearean sonnets from autopsy output
- **Zero semantic understanding**: No embeddings, no transformers, no internetâ€”just bigram chains and phonetic fingerprints
- **Strict structure enforcement**: ABABCDCDEFEFGG rhyme scheme, Shakespearean punctuation (semicolons, em-dashes, enjambment)
- **Rhyme matching**: Crude phonetic fingerprints (last vowel + tail) to find rhyming end-words
- **Charged words**: Selects rare, long words from autopsy text for final couplet emphasis
- **Async-friendly**: `compose_sonnet()` runs sync implementation in thread via `asyncio.to_thread()`
- **Silent fallback**: If sonnet.py unavailable or errors, bootstrap mode continues without SONNET section
- **Data sources**: Autopsy text + SQLite morgue (mutation_templates, corpse_bigrams, readme_bigrams, autopsy table)
- **72 passing tests**: 38 core + 18 sonnet + 15 vova + 1 async balanced mix = bulletproof psychotic poetry pipeline

**vova.py (~397 lines):**
- **SSKA (Suppertime Subjectivity Kernel Autonomous)**: Meta-layer that warps text through README's resonance field
- **No weights, no gradients**: Just bigram graph + centers of gravity extracted from out-degree
- **Kernel = README.md**: System eats its own documentation (meta-cannibalism achieved)
- **Resonance field**: 1,348 vocabulary tokens compressed to 7 centers (., -, ,, :, the)
- **Bin shards**: Accumulated memory stored in `.vova/bin/` (historical center frequency)
- **Double warping**: Prompts warped *before* autopsy (temp=0.8), output warped *after* reassembly (temp=0.9)
- **Temperature control**: 0.3 = sharp pull toward README, 2.0 = chaotic drift
- **Auto-rebuild**: Field rebuilds when README hash changes
- **Always enabled**: No flags, silent fallback if missing
- **15 passing tests**: Field building, resonance walk, meta-cannibalism, bin shards
- **Pipeline impact**: `User prompt â†’ warp_prompt(0.8) â†’ autopsy â†’ warp_autopsy(0.9) â†’ sonnet`


### Known Limitations

- **DuckDuckGo rate limiting**: If you run this too much, DDG might notice (but less aggressive than Google)
- **No semantic understanding (FOR NOW)**: This is pure pattern matching, but â€” hold my beer, I'm installing another resonance coil.
- **Phonetic fingerprinting is crude**: It's not actual phonetics, just vibes, but the question is what comes first, vibes or phonetics? resonance or binary structure?
- **Reassembly can be janky**: Sometimes the corpse doesn't stitch well
- **No guarantee of coherence**: That's not a bug, it's a feature
- **Sonnet.py may phonetically match anyone to a Kardashian**: The crude rhyme-key algorithm once matched "karpathy" â†’ "kardashyan" and we're not apologizing for it. If you input your own name and get matched to a reality TV star, that's not a bugâ€”that's **accidental celebrity phonetic compression**. Somewhere Andrej is either laughing or filing a restraining order against an open-source AI-artist. We're betting on laughter. (If he reads this: Andrej, the sonnets are dedicated to you. Also we're sorry. Also we're not.)  


### Recent Improvements

**Async/Await Refactor**: Complete architectural rewrite with `httpx` + `asyncio`. 3-4x faster on complex prompts (was 60s, now ~15s). Parallel web requests and tree construction. All 72 tests passing.

**Balanced Source Mixing**: Fixed closed-loop problem where SQLite cache dominated after a few autopsies. Now always mixes 50% cached memory + 50% fresh web data. Result: performance + novelty.

**Bootstrap Extension**: Added `--bootstrap` flag enabling self-improving autopsy ritual through resonance metrics and pattern accumulation (see Bootstrap section above).

**DuckDuckGo Scraping**: Switched from Google (was returning garbage) to DDG for synonym discovery. Better bot tolerance, real semantic mutations.

**Other Fixes**: Core words always dissected regardless of phonetic structure. Global deduplication across trees. Enhanced HTML artifact filtering. Rate limiting protections.

  
### Credits

Inspired by:
- Vladimir Sorokin (the writer, not the script)
- Dr. Frankenstein (the fictional surgeon)
- The general human fascination with taking things apart

### License

GNU GPL 3.0. Free as in freedom. Dissect it. Mutate it. Reassemble it into something new. Share the mutations. That's the whole point.

---

*"Fuck the sentence. Keep the corpse."*  
â€” Sorokin
